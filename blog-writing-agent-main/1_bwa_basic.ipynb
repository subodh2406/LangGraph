{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36276464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5141deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fae9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac1a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # reducer: results from workers get concatenated automatically\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594342a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda73e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict: #Take a topic → create a structured plan → hand that plan to workers.\n",
    "\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"Create a blog plan with 5-7 sections on the following topic.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9ce5998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\", \n",
    "                {\n",
    "                 \"task\": task, \n",
    "                 \"topic\": state[\"topic\"], \n",
    "                 \"plan\": state[\"plan\"]\n",
    "                }\n",
    "            )\n",
    "            for task in state[\"plan\"].tasks\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1a87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    # payload contains what we sent\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write one clean Markdown section.\"),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {blog_title}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Brief: {task.brief}\\n\\n\"\n",
    "                    \"Return only the section content in Markdown.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a37817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "def reducer(state: State) -> dict:\n",
    "    \n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "\n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # ---- save to file ----\n",
    "    filename = title.lower().replace(\" \", \"_\") + \".md\"\n",
    "    # safe_title = re.sub(r\"[^\\w\\s-]\", \"\", title)  # remove special characters\n",
    "    # filename = safe_title.lower().replace(\" \", \"_\") + \".md\"\n",
    "    output_path = Path(filename)\n",
    "    output_path.write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef584a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x25d1a785010>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = StateGraph(State)\n",
    "g.add_node(\"orchestrator\", orchestrator)\n",
    "g.add_node(\"worker\", worker)\n",
    "g.add_node(\"reducer\", reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d7dafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000025D1A7FBFB0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.add_edge(START, \"orchestrator\")\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app\n",
    "\n",
    "# The orchestrator creates a plan with multiple tasks.\n",
    "\n",
    "# We don’t know how many tasks there will be until runtime.\n",
    "\n",
    "# So we can’t connect a fixed edge to just one worker.\n",
    "\n",
    "# The conditional edge calls fanout(state) at runtime, which returns one Send per task.\n",
    "\n",
    "# LangGraph then runs all these worker nodes in parallel.\n",
    "\n",
    "# ✅ In short: Conditional edge = “decide how many workers to run, based on the plan the orchestrator made.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd0ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f860e855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'plan': Plan(blog_title='Understanding Self-Attention: The Heart of Modern Neural Networks', tasks=[Task(id=1, title='Introduction to Self-Attention', brief='Explain the concept of attention in neural networks and introduce self-attention as a mechanism that allows models to weigh the importance of different parts of input data.'), Task(id=2, title='The Mechanics of Self-Attention', brief='Detail how self-attention works step-by-step, including the concepts of queries, keys, and values, and how attention scores are computed and applied.'), Task(id=3, title='Self-Attention in Transformer Models', brief='Discuss the role of self-attention in Transformer architectures, highlighting how it enables efficient handling of sequence data without recurrent structures.'), Task(id=4, title='Advantages of Self-Attention', brief='Explore benefits such as parallelizability, capturing long-range dependencies, and flexibility compared to traditional RNNs or CNNs.'), Task(id=5, title='Applications of Self-Attention', brief='Highlight areas where self-attention is transforming AI, including natural language processing, computer vision, and more.'), Task(id=6, title='Challenges and Future Directions', brief='Address current limitations of self-attention mechanisms and potential research directions to overcome them.'), Task(id=7, title='Conclusion', brief='Summarize key takeaways about self-attention and encourage further exploration and learning.')]),\n",
       " 'sections': ['## Introduction to Self-Attention\\n\\nIn the realm of neural networks, **attention** mechanisms have revolutionized how models process and interpret data. At its core, attention allows a model to dynamically focus on the most relevant parts of the input when generating an output, rather than treating all input information equally. This ability to weigh different pieces of information based on their importance has improved performance in a wide variety of tasks, from language translation to image recognition.\\n\\n**Self-attention** takes this concept a step further by enabling a model to assess the relationships within a single sequence of data. Instead of relying on external guidance, self-attention allows each element in the input to interact with every other element, learning which parts are most significant in the context of the whole. For example, in natural language processing, this means a word in a sentence can directly attend to other words—capturing context, dependencies, and nuances that traditional models might miss.\\n\\nBy enabling this internal weighing mechanism, self-attention serves as the heart of many modern neural architectures, such as the Transformer model. It empowers models to build rich, context-aware representations of data, facilitating improved understanding and generation capabilities across diverse applications.',\n",
       "  \"## The Mechanics of Self-Attention\\n\\nSelf-attention is a fundamental mechanism that allows neural networks, particularly transformers, to weigh the importance of different parts of an input sequence when processing data. Here's a step-by-step breakdown of how self-attention works:\\n\\n1. **Input Representation**  \\n   Each element in the input sequence (e.g., words in a sentence) is first represented as a vector, typically obtained via embeddings. These vectors capture semantic information about the tokens.\\n\\n2. **Creating Queries, Keys, and Values**  \\n   For each input vector, the model generates three distinct vectors through learned linear transformations:\\n   - **Query (Q):** Represents the current element seeking relevant information.\\n   - **Key (K):** Represents each element’s identity that can be matched against queries.\\n   - **Value (V):** Contains the actual information or content to be aggregated.\\n\\n3. **Computing Attention Scores**  \\n   The core idea is to measure how well each query matches every key. This is done by:\\n   - Taking the dot product between the query vector and each key vector.\\n   - Scaling the result by the square root of the dimension of the keys to stabilize gradients.\\n   - Applying a softmax function to convert the scores into probabilities that sum to one.\\n\\n   Mathematically:  \\n   \\\\[\\n   \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{Q K^T}{\\\\sqrt{d_k}}\\\\right) V\\n   \\\\]\\n\\n4. **Weighted Summation of Values**  \\n   The softmax scores act as weights that determine how much attention the model should pay to each element in the sequence. These weights are then used to compute a weighted sum of the value vectors, effectively aggregating contextual information relative to the current token.\\n\\n5. **Output**  \\n   The resulting vector after this weighted sum replaces or augments the original token representation, now enriched with contextual awareness from the entire sequence.\\n\\nBy repeating this process for every token in the sequence, the self-attention mechanism enables the model to capture intricate relationships and dependencies regardless of distance, empowering it to understand context in a dynamic and flexible way.\",\n",
       "  '## Self-Attention in Transformer Models\\n\\nSelf-attention is a fundamental mechanism that powers Transformer architectures, revolutionizing how neural networks process sequential data. Unlike traditional recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) that rely on sequential processing, Transformers use self-attention to capture relationships between all elements in a sequence simultaneously. This parallelism significantly accelerates training and inference.\\n\\nAt its core, self-attention computes a weighted representation of the input sequence by measuring the relevance of each token to every other token. This is achieved through three learned vectors for each token: *query*, *key*, and *value*. The attention scores are calculated by taking the dot product between queries and keys, normalizing them with a softmax function to obtain attention weights, and then multiplying these weights by the values. This process allows the model to dynamically focus on different parts of the sequence when encoding each token, effectively capturing long-range dependencies and contextual information.\\n\\nBy replacing recurrence with self-attention, Transformers efficiently handle sequences in parallel, overcoming the bottlenecks of traditional models. This not only improves scalability and training speed but also enhances the model’s ability to understand complex patterns in natural language, making self-attention the heart of cutting-edge neural architectures like BERT, GPT, and many others.',\n",
       "  '## Advantages of Self-Attention\\n\\nSelf-attention mechanisms have revolutionized the way neural networks process sequential data, offering several key advantages over traditional architectures like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs):\\n\\n- **Parallelizability**: Unlike RNNs, which process data sequentially, self-attention allows simultaneous computation across all positions in the input sequence. This parallel processing significantly speeds up training and inference times, making it well-suited for handling large datasets and complex models.\\n\\n- **Capturing Long-Range Dependencies**: Traditional RNNs and CNNs often struggle with long-distance relationships due to vanishing gradients or limited receptive fields. Self-attention naturally models dependencies regardless of their positions by directly attending to all elements in the sequence, enabling the model to learn context from distant tokens effectively.\\n\\n- **Flexibility and Adaptability**: Self-attention dynamically adjusts the importance of different parts of the input data based on the context, allowing the network to focus on the most relevant information at each step. This adaptability grants the model superior performance across varied tasks, from language translation to image recognition.\\n\\nTogether, these benefits make self-attention a powerful core component in modern neural networks, driving advances in fields such as natural language processing, computer vision, and beyond.',\n",
       "  '## Applications of Self-Attention\\n\\nSelf-attention has rapidly become a cornerstone in the field of artificial intelligence, driving breakthroughs across numerous domains. Its ability to dynamically weigh the importance of different parts of input data enables models to capture complex dependencies and contextual information more effectively than traditional architectures. Here are some key areas where self-attention is transforming AI:\\n\\n### Natural Language Processing (NLP)\\n\\nSelf-attention is at the heart of transformer models like BERT, GPT, and T5, which have revolutionized NLP tasks such as language translation, sentiment analysis, summarization, and question answering. By allowing models to focus on relevant words or phrases regardless of their position in a sentence, self-attention enhances understanding of context and nuance in human language.\\n\\n### Computer Vision\\n\\nIn computer vision, self-attention mechanisms facilitate the development of Vision Transformers (ViTs), which rival or surpass convolutional neural networks (CNNs) in image recognition, object detection, and segmentation. Self-attention helps models capture long-range dependencies in images, improving the interpretation of complex visual patterns and relationships.\\n\\n### Speech Processing\\n\\nSelf-attention is driving advancements in speech recognition and synthesis by better modeling temporal dependencies in audio signals. This leads to more accurate transcription, natural-sounding text-to-speech systems, and enhanced speaker identification.\\n\\n### Reinforcement Learning\\n\\nIn reinforcement learning, self-attention improves agents’ decision-making by enabling them to attend to essential events over varying time horizons. This capability allows for better strategy formation in complex environments, such as games or robotics.\\n\\n### Multimodal Learning\\n\\nSelf-attention also enables the fusion of information from multiple data modalities—like text, images, and audio—helping create more holistic and context-aware AI systems. This is crucial for tasks such as image captioning, video understanding, and interactive AI assistants.\\n\\nIn summary, self-attention empowers AI models to understand and process data in more flexible and context-sensitive ways, making it a pivotal innovation that continues to expand the horizons of machine intelligence.',\n",
       "  '## Challenges and Future Directions\\n\\nDespite its transformative impact on natural language processing and beyond, the self-attention mechanism faces several challenges that motivate ongoing research:\\n\\n- **Computational Complexity**: Self-attention scales quadratically with input sequence length, making it computationally intensive and memory-hungry for long sequences. This limits practical use in tasks involving very large inputs such as long documents or high-resolution images.\\n\\n- **Lack of Explicit Structure**: Unlike convolutional or recurrent networks, self-attention does not inherently encode positional or hierarchical structure, often requiring additional mechanisms like positional encodings or relative attention to capture sequence order and structured relationships effectively.\\n\\n- **Interpretability**: While attention weights provide some insight into model focus, they do not always correlate clearly with model decisions, challenging efforts to interpret and trust model predictions based on self-attention.\\n\\n- **Robustness and Generalization**: Self-attention models can be sensitive to input perturbations and may overfit to training distribution biases, requiring research into more robust architectures and training strategies.\\n\\n### Future Directions\\n\\nTo address these limitations, the research community is exploring various promising avenues:\\n\\n- **Efficient Attention Variants**: Development of sparse, low-rank, or kernel-based attention methods aim to reduce computational and memory demands, enabling scalable models for longer sequences and larger modalities.\\n\\n- **Improved Structural Priors**: Integrating syntactic, semantic, or hierarchical structures through novel positional encodings or hybrid architectures seeks to enhance the model’s ability to capture complex dependencies efficiently.\\n\\n- **Better Interpretability Tools**: Advances in explainable AI applied to attention-based models aim to clarify the connection between attention distributions and model reasoning processes.\\n\\n- **Robustness Enhancements**: Techniques such as adversarial training, data augmentation, and regularization tailored for self-attention networks are being investigated to improve their reliability and generalization across diverse tasks.\\n\\nAs self-attention continues to evolve, overcoming these challenges will be key to unlocking its full potential across AI applications, driving more efficient, interpretable, and robust neural networks in the future.',\n",
       "  '## Conclusion\\n\\nSelf-attention has revolutionized the way neural networks process and understand data by enabling models to dynamically focus on different parts of input sequences. Its ability to capture long-range dependencies and contextual relationships has been crucial in advancing natural language processing, computer vision, and beyond. As the foundation of architectures like Transformers, self-attention continues to drive state-of-the-art performance across diverse applications. By grasping the core principles of self-attention, you open the door to exploring cutting-edge AI models and contributing to this rapidly evolving field. Keep experimenting, learning, and pushing the boundaries of what these powerful mechanisms can achieve!'],\n",
       " 'final': \"# Understanding Self-Attention: The Heart of Modern Neural Networks\\n\\n## Introduction to Self-Attention\\n\\nIn the realm of neural networks, **attention** mechanisms have revolutionized how models process and interpret data. At its core, attention allows a model to dynamically focus on the most relevant parts of the input when generating an output, rather than treating all input information equally. This ability to weigh different pieces of information based on their importance has improved performance in a wide variety of tasks, from language translation to image recognition.\\n\\n**Self-attention** takes this concept a step further by enabling a model to assess the relationships within a single sequence of data. Instead of relying on external guidance, self-attention allows each element in the input to interact with every other element, learning which parts are most significant in the context of the whole. For example, in natural language processing, this means a word in a sentence can directly attend to other words—capturing context, dependencies, and nuances that traditional models might miss.\\n\\nBy enabling this internal weighing mechanism, self-attention serves as the heart of many modern neural architectures, such as the Transformer model. It empowers models to build rich, context-aware representations of data, facilitating improved understanding and generation capabilities across diverse applications.\\n\\n## The Mechanics of Self-Attention\\n\\nSelf-attention is a fundamental mechanism that allows neural networks, particularly transformers, to weigh the importance of different parts of an input sequence when processing data. Here's a step-by-step breakdown of how self-attention works:\\n\\n1. **Input Representation**  \\n   Each element in the input sequence (e.g., words in a sentence) is first represented as a vector, typically obtained via embeddings. These vectors capture semantic information about the tokens.\\n\\n2. **Creating Queries, Keys, and Values**  \\n   For each input vector, the model generates three distinct vectors through learned linear transformations:\\n   - **Query (Q):** Represents the current element seeking relevant information.\\n   - **Key (K):** Represents each element’s identity that can be matched against queries.\\n   - **Value (V):** Contains the actual information or content to be aggregated.\\n\\n3. **Computing Attention Scores**  \\n   The core idea is to measure how well each query matches every key. This is done by:\\n   - Taking the dot product between the query vector and each key vector.\\n   - Scaling the result by the square root of the dimension of the keys to stabilize gradients.\\n   - Applying a softmax function to convert the scores into probabilities that sum to one.\\n\\n   Mathematically:  \\n   \\\\[\\n   \\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{Q K^T}{\\\\sqrt{d_k}}\\\\right) V\\n   \\\\]\\n\\n4. **Weighted Summation of Values**  \\n   The softmax scores act as weights that determine how much attention the model should pay to each element in the sequence. These weights are then used to compute a weighted sum of the value vectors, effectively aggregating contextual information relative to the current token.\\n\\n5. **Output**  \\n   The resulting vector after this weighted sum replaces or augments the original token representation, now enriched with contextual awareness from the entire sequence.\\n\\nBy repeating this process for every token in the sequence, the self-attention mechanism enables the model to capture intricate relationships and dependencies regardless of distance, empowering it to understand context in a dynamic and flexible way.\\n\\n## Self-Attention in Transformer Models\\n\\nSelf-attention is a fundamental mechanism that powers Transformer architectures, revolutionizing how neural networks process sequential data. Unlike traditional recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) that rely on sequential processing, Transformers use self-attention to capture relationships between all elements in a sequence simultaneously. This parallelism significantly accelerates training and inference.\\n\\nAt its core, self-attention computes a weighted representation of the input sequence by measuring the relevance of each token to every other token. This is achieved through three learned vectors for each token: *query*, *key*, and *value*. The attention scores are calculated by taking the dot product between queries and keys, normalizing them with a softmax function to obtain attention weights, and then multiplying these weights by the values. This process allows the model to dynamically focus on different parts of the sequence when encoding each token, effectively capturing long-range dependencies and contextual information.\\n\\nBy replacing recurrence with self-attention, Transformers efficiently handle sequences in parallel, overcoming the bottlenecks of traditional models. This not only improves scalability and training speed but also enhances the model’s ability to understand complex patterns in natural language, making self-attention the heart of cutting-edge neural architectures like BERT, GPT, and many others.\\n\\n## Advantages of Self-Attention\\n\\nSelf-attention mechanisms have revolutionized the way neural networks process sequential data, offering several key advantages over traditional architectures like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs):\\n\\n- **Parallelizability**: Unlike RNNs, which process data sequentially, self-attention allows simultaneous computation across all positions in the input sequence. This parallel processing significantly speeds up training and inference times, making it well-suited for handling large datasets and complex models.\\n\\n- **Capturing Long-Range Dependencies**: Traditional RNNs and CNNs often struggle with long-distance relationships due to vanishing gradients or limited receptive fields. Self-attention naturally models dependencies regardless of their positions by directly attending to all elements in the sequence, enabling the model to learn context from distant tokens effectively.\\n\\n- **Flexibility and Adaptability**: Self-attention dynamically adjusts the importance of different parts of the input data based on the context, allowing the network to focus on the most relevant information at each step. This adaptability grants the model superior performance across varied tasks, from language translation to image recognition.\\n\\nTogether, these benefits make self-attention a powerful core component in modern neural networks, driving advances in fields such as natural language processing, computer vision, and beyond.\\n\\n## Applications of Self-Attention\\n\\nSelf-attention has rapidly become a cornerstone in the field of artificial intelligence, driving breakthroughs across numerous domains. Its ability to dynamically weigh the importance of different parts of input data enables models to capture complex dependencies and contextual information more effectively than traditional architectures. Here are some key areas where self-attention is transforming AI:\\n\\n### Natural Language Processing (NLP)\\n\\nSelf-attention is at the heart of transformer models like BERT, GPT, and T5, which have revolutionized NLP tasks such as language translation, sentiment analysis, summarization, and question answering. By allowing models to focus on relevant words or phrases regardless of their position in a sentence, self-attention enhances understanding of context and nuance in human language.\\n\\n### Computer Vision\\n\\nIn computer vision, self-attention mechanisms facilitate the development of Vision Transformers (ViTs), which rival or surpass convolutional neural networks (CNNs) in image recognition, object detection, and segmentation. Self-attention helps models capture long-range dependencies in images, improving the interpretation of complex visual patterns and relationships.\\n\\n### Speech Processing\\n\\nSelf-attention is driving advancements in speech recognition and synthesis by better modeling temporal dependencies in audio signals. This leads to more accurate transcription, natural-sounding text-to-speech systems, and enhanced speaker identification.\\n\\n### Reinforcement Learning\\n\\nIn reinforcement learning, self-attention improves agents’ decision-making by enabling them to attend to essential events over varying time horizons. This capability allows for better strategy formation in complex environments, such as games or robotics.\\n\\n### Multimodal Learning\\n\\nSelf-attention also enables the fusion of information from multiple data modalities—like text, images, and audio—helping create more holistic and context-aware AI systems. This is crucial for tasks such as image captioning, video understanding, and interactive AI assistants.\\n\\nIn summary, self-attention empowers AI models to understand and process data in more flexible and context-sensitive ways, making it a pivotal innovation that continues to expand the horizons of machine intelligence.\\n\\n## Challenges and Future Directions\\n\\nDespite its transformative impact on natural language processing and beyond, the self-attention mechanism faces several challenges that motivate ongoing research:\\n\\n- **Computational Complexity**: Self-attention scales quadratically with input sequence length, making it computationally intensive and memory-hungry for long sequences. This limits practical use in tasks involving very large inputs such as long documents or high-resolution images.\\n\\n- **Lack of Explicit Structure**: Unlike convolutional or recurrent networks, self-attention does not inherently encode positional or hierarchical structure, often requiring additional mechanisms like positional encodings or relative attention to capture sequence order and structured relationships effectively.\\n\\n- **Interpretability**: While attention weights provide some insight into model focus, they do not always correlate clearly with model decisions, challenging efforts to interpret and trust model predictions based on self-attention.\\n\\n- **Robustness and Generalization**: Self-attention models can be sensitive to input perturbations and may overfit to training distribution biases, requiring research into more robust architectures and training strategies.\\n\\n### Future Directions\\n\\nTo address these limitations, the research community is exploring various promising avenues:\\n\\n- **Efficient Attention Variants**: Development of sparse, low-rank, or kernel-based attention methods aim to reduce computational and memory demands, enabling scalable models for longer sequences and larger modalities.\\n\\n- **Improved Structural Priors**: Integrating syntactic, semantic, or hierarchical structures through novel positional encodings or hybrid architectures seeks to enhance the model’s ability to capture complex dependencies efficiently.\\n\\n- **Better Interpretability Tools**: Advances in explainable AI applied to attention-based models aim to clarify the connection between attention distributions and model reasoning processes.\\n\\n- **Robustness Enhancements**: Techniques such as adversarial training, data augmentation, and regularization tailored for self-attention networks are being investigated to improve their reliability and generalization across diverse tasks.\\n\\nAs self-attention continues to evolve, overcoming these challenges will be key to unlocking its full potential across AI applications, driving more efficient, interpretable, and robust neural networks in the future.\\n\\n## Conclusion\\n\\nSelf-attention has revolutionized the way neural networks process and understand data by enabling models to dynamically focus on different parts of input sequences. Its ability to capture long-range dependencies and contextual relationships has been crucial in advancing natural language processing, computer vision, and beyond. As the foundation of architectures like Transformers, self-attention continues to drive state-of-the-art performance across diverse applications. By grasping the core principles of self-attention, you open the door to exploring cutting-edge AI models and contributing to this rapidly evolving field. Keep experimenting, learning, and pushing the boundaries of what these powerful mechanisms can achieve!\\n\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (langgraph)",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
