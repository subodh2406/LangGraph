{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63538ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e35ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # keep if Tavily provides; DO NOT rely on it\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb95c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555c7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfac783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "    \n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71511297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily) \n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
    "  If missing or unclear, set published_at=null. Do NOT guess.\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "\n",
    "    # take the first 10 queries from state\n",
    "    queries = (state.get(\"queries\", []) or [])\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(content=f\"Raw results:\\n{raw_results}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return {\"evidence\": list(dedup.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7408e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book:\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ee5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d79c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard:\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book:\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true:\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [(task.id, section_md)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d783e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8) Reducer (merge + save)\n",
    "# -----------------------------\n",
    "def reducer_node(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    final_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622776f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x118fa6310>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_node)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9537bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str):\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"sections\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d94f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/gh59pbb174b94mbxsb8xb9qc0000gn/T/ipykernel_36817/4168389408.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(max_results=max_results)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'State of Multimodal LLMs in 2026',\n",
       " 'mode': 'open_book',\n",
       " 'needs_research': True,\n",
       " 'queries': ['State of multimodal large language models in 2026',\n",
       "  'Latest advancements in multimodal LLM technology as of 2026',\n",
       "  'Top multimodal LLMs released in 2026',\n",
       "  'Use cases for multimodal LLMs in 2026',\n",
       "  'Challenges faced by multimodal LLMs in 2026',\n",
       "  'Performance benchmarks of multimodal LLMs in 2026',\n",
       "  'Industry adoption of multimodal LLMs in 2026'],\n",
       " 'evidence': [EvidenceItem(title='Top 9 Large Language Models as of January 2026 - Shakudo', url='https://www.shakudo.io/blog/top-9-large-language-models', published_at=None, snippet='Meta continues to be a leader in the LLM space with its state-of-the-art Llama models, prioritizing an open-source approach. The latest major release is Llama 4, which includes natively multimodal models like Llama 4 Scout and Llama 4 Maverick. These models can process text, images, and short videos, and are built on a Mixture-of-Experts (MoE) architecture for increased efficiency.', source=None),\n",
       "  EvidenceItem(title='Top LLMs and AI Trends for 2026 | Clarifai Industry Guide', url='https://www.clarifai.com/blog/llms-and-ai-trends', published_at=None, snippet='Several trends set the stage for 2026: Multimodal models can parse text, images, audio and even video. Zapier’s analysis notes that reasoning models and large multimodal models (LMMs) are the two most important developments. Companies like Google (Gemini), OpenAI and Mistral are racing to offer native multimodal support. Extended context windows: Models like GPT‑4 Turbo and Claude 3 Sonnet handle hundreds of pages of text. Next‑generation models promise context windows up to 200 k tokens or beyond, enabling them to read entire knowledge bases or code repositories.', source=None),\n",
       "  EvidenceItem(title='30 of the best large language models in 2026 - TechTarget', url='https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models', published_at=None, snippet=\"Falcon transformer-based models developed by the Technology Innovation Institute is open source and has multilingual capabilities. Falcon 2 is available in an 11-billion-parameter version that provides multimodal capabilities for both text and vision. Falcon 3 is available in several sizes ranging from 1 to 10 billion parameters. Gemini is Google's group of LLMs that power the company's chatbot of the same name. The model replaced Palm to power the chatbot, which was rebranded from Bard to Gemini upon the model switch. Gemini models are multimodal to handle images, audio and video as well as text.\", source=None),\n",
       "  EvidenceItem(title='The Fastest Open Source Multimodal Models in 2026 - SiliconFlow', url='https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models', published_at=None, snippet='Qwen2.5-VL-32B-Instruct is a multimodal large language model released by the Qwen team, part of the Qwen2.5-VL series. This model excels at analyzing texts, charts, icons, graphics, and layouts within images. It acts as a visual agent that can reason and dynamically direct tools, capable of computer and phone use. The model can accurately localize objects in images and generate structured outputs for data like invoices and tables, with enhanced mathematical and problem-solving abilities through reinforcement learning.', source=None),\n",
       "  EvidenceItem(title='Large Multimodal Models (LMMs) vs LLMs in 2026', url='https://research.aimultiple.com/large-multimodal-models/', published_at=None, snippet='Recent advancements in multimodal models have introduced new capabilities and efficiencies in AI development. One notable feature is its Real-time Voice Mode, which can adjust tone, pace, and style according to user instructions. This creates a more natural and adaptive conversational experience. Visual processing has also improved, reducing hallucinations in interpreting or generating images, diagrams, and charts. Another advancement lies in its memory capabilities, which allow the system to recall earlier inputs and maintain context over extended interactions.', source=None),\n",
       "  EvidenceItem(title='Top 10 Multimodal LLMs to Explore in 2026 - Analytics Vidhya', url='https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/', published_at=None, snippet='Multimodal LLMs (MLLMs) are rapidly transforming in 2026 with the capabilities to process text, images, audio, and video. This has enhanced user experience and expanded the applications of AI across various industries. The trends that are major among them are the advent of open-source models, increased investment in AI infrastructure, and developing special models for specific tasks. All these collectively drive AI deeper into various industries and make it a fundamental technology in modern technology.', source=None),\n",
       "  EvidenceItem(title='Top 10 Most Popular LLMs in 2026 - ZenMux', url='https://zenmux.ai/blog/top-10-most-popular-llms-in-2026', published_at='2026-01-05', snippet='Powered by Google DeepMind’s Gemini 3 family, this model sets a new benchmark for multimodal reasoning and creativity. Gemini 3 Pro integrates text, image, audio, video, and code understanding—delivering world‑leading performance in scientific reasoning (GPQA Diamond 91.9), mathematics (AIME 2026 95.0), and multimodal comprehension (MMMU‑Pro 81.0). Context Window: Up to 2 million tokens Modes: Deep multimodal processing with enhanced agentic capabilities Distinct Features: Deep Think mode for parallel reasoning and strategic problem solving Use Cases: Enterprise search, software development assistants, scientific analysis, design generation, and video understanding', source=None),\n",
       "  EvidenceItem(title='Top LLMs To Use in 2026: Our Best Picks - Splunk', url='https://www.splunk.com/en_us/blog/learn/llms-best-to-use.html', published_at=None, snippet='All Llama 4 models are built on a Mixture-of-Experts (MoE) architecture, balancing speed, efficiency, and performance. They were trained from the ground up to handle both text and visual inputs using an early fusion technique, which blends vision and language data during training. Meta also improved performance through a fine-tuned MetaCLIP-based vision encoder and MetaP-optimized settings. The models were trained using a new post-training pipeline including Supervised Fine-Tuning, Reinforcement Learning, and Direct Preference Optimization.', source=None)],\n",
       " 'plan': Plan(blog_title='State of Multimodal LLMs in 2026: Overview and Industry Implications', audience='Developers interested in the latest trends and capabilities of large multimodal language models (LLMs)', tone='Informative and analytical', blog_kind='news_roundup', constraints=[], tasks=[Task(id=1, title='Overview of Leading Multimodal LLM Architectures in 2026', goal='Provide a snapshot of the top multimodal large language models and their architectural innovations in 2026.', bullets=[\"Summarize key models like Meta's Llama 4 (Scout and Maverick) using Mixture-of-Experts architecture for efficiency.\", \"Highlight Google's Gemini series including Gemini 3 Pro with deep multimodal processing and agentic capabilities.\", 'Describe Falcon models from Technology Innovation Institute focusing on their transformer base and multilingual, multimodal capabilities.', 'Note open-source models like Qwen2.5-VL-32B-Instruct emphasizing visual reasoning and structured data extraction.', 'Compare architectural styles such as MoE, transformer-based, and fusion techniques for multimodal input processing.'], target_words=350, tags=['architecture', 'models', 'overview'], requires_research=True, requires_citations=True, requires_code=False), Task(id=2, title='Expanding Modalities: Text, Vision, Audio, and Video', goal='Explain the types of modalities supported by multimodal LLMs in 2026 and their practical implications.', bullets=['Document the ability of models like Gemini and Llama 4 to natively process text, images, audio, and video inputs.', 'Discuss the integration of audio and real-time voice modes to adjust tone and style during interaction.', 'Summarize how video processing and image understanding have improved, including object localization and diagram interpretation.', 'Highlight limitations and challenges in modality integration, such as hallucination reduction and synchronization across modalities.', 'Review industry applications enabled by rich modality support, including multimedia search, scientific analysis, and design generation.'], target_words=400, tags=['modalities', 'capabilities', 'applications'], requires_research=True, requires_citations=True, requires_code=False), Task(id=3, title='Context Window Expansion and Memory Enhancements', goal='Describe advancements in extended context window sizes and memory systems in 2026 multimodal LLMs.', bullets=['Explain current context window sizes reaching up to 2 million tokens in models like Gemini 3 Pro.', 'Discuss how extended context supports entire knowledge bases, code repositories, and long document analysis.', 'Detail memory improvements enabling sustained context tracking and recall of earlier conversation or input elements.', 'Highlight potential developer benefits such as building more contextually aware assistants and analytical tools.', 'Mention performance considerations and trade-offs of very large context windows on latency and compute resources.'], target_words=350, tags=['context', 'memory', 'performance'], requires_research=True, requires_citations=True, requires_code=False), Task(id=4, title='Open-Source Multimodal Models and Democratization of AI', goal='Summarize the impact of open-source multimodal LLMs on accessibility and AI innovation.', bullets=['Identify key open-source multimodal models like Falcon series and Qwen2.5-VL-32B-Instruct and their unique capabilities.', 'Discuss how open-source licensing fosters AI research, experimentation, and integration in diverse applications.', \"Highlight performance benefits such as speed and specialization in models like Qwen2.5's ability to reason visually and interact with tools.\", 'Consider ecosystem effects including community contributions, forks, model tuning, and deployment versatility.', 'Address potential drawbacks such as maintenance overhead, security considerations, and model convergence challenges.'], target_words=320, tags=['opensource', 'community', 'models'], requires_research=True, requires_citations=True, requires_code=False), Task(id=5, title='Advancements in Reasoning and Agentic Capabilities', goal='Outline enhancements in reasoning and autonomous decision-making features of multimodal LLMs in 2026.', bullets=['Detail ‘Deep Think’ mode in Gemini 3 Pro enabling parallel, strategic, and complex problem solving.', 'Explain reinforcement learning techniques used to boost mathematical and problem-solving skills as in Qwen2.5.', 'Describe agentic capabilities allowing models to dynamically direct tools and take multi-step actions in workflows.', 'Review improvements in factual accuracy and hallucination reduction in multimodal contexts, especially image and diagram understanding.', 'Elaborate on use cases enabled by enhanced reasoning, such as scientific analysis, software development assistants, and enterprise search.'], target_words=400, tags=['reasoning', 'agentic', 'features'], requires_research=True, requires_citations=True, requires_code=False), Task(id=6, title='Performance, Efficiency, and Architectural Trade-offs', goal='Analyze the performance and computational efficiency aspects of multimodal LLMs in 2026.', bullets=['Discuss Mixture-of-Experts (MoE) architectures in Llama 4 models balancing speed and resource usage.', 'Compare inference speeds and hardware requirements among open-source and proprietary multimodal models.', 'Explain training pipeline innovations such as supervised fine-tuning, reinforcement learning, and direct preference optimization.', 'Highlight cost implications for enterprise deployment and scaling considerations for very large models.', 'Mention performance benchmarking metrics relevant for multimodal tasks and their current state in 2026.'], target_words=350, tags=['performance', 'efficiency', 'systems'], requires_research=True, requires_citations=True, requires_code=False), Task(id=7, title='Applications and Industry Impact of Multimodal LLMs in 2026', goal='Summarize how multimodal LLMs are transforming industries and enabling new AI-driven applications.', bullets=['Overview of applications in scientific research, multimedia search, software development, and design creation.', 'Explain use in real-time video understanding, enterprise search solutions, and interactive agents.', 'Discuss influence on AI democratization and addressing domain-specific challenges with specialized models.', 'Review broader societal and business impacts such as automation, knowledge work augmentation, and accessibility improvements.', 'Highlight emerging fields leveraging multimodal LLM capabilities like augmented reality and robotics interface integration.'], target_words=400, tags=['applications', 'industry', 'impact'], requires_research=True, requires_citations=True, requires_code=False)]),\n",
       " 'sections': [(1,\n",
       "   '## Overview of Leading Multimodal LLM Architectures in 2026\\n\\nIn 2026, the landscape of multimodal large language models (LLMs) is defined by diverse architectural innovations aimed at enhancing efficiency, multimodal understanding, and task generalization. Here’s a snapshot of the foremost models and their key architectural traits:\\n\\n- **Meta’s Llama 4 Series (Scout and Maverick)**  \\n  Meta’s Llama 4 variants leverage a **Mixture-of-Experts (MoE)** architecture, where different expert subnetworks are dynamically activated per input. This approach significantly improves computation efficiency and scalability while maintaining performance. Scout and Maverick specialize in balancing general language understanding with refined multimodal capabilities, enabling them to process complex text and visual inputs simultaneously without excessive resource consumption ([Source](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\n- **Google’s Gemini Series**  \\n  Gemini models, particularly the **Gemini 3 Pro**, integrate deep multimodal processing layers with advanced agentic capabilities, allowing them to perform autonomous reasoning and decision-making across text, images, and other modalities. Their architecture builds on a high-capacity Transformer backbone fused with specialized cross-modal attention mechanisms that tightly connect visual and textual streams. This fusion facilitates nuanced contextual understanding beyond simple feature concatenation ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n- **Falcon Models from Technology Innovation Institute**  \\n  The Falcon family uses a robust **transformer-centric design** tuned for multilingual and multimodal data. Emphasis on large-scale text and image tokenization techniques makes Falcon models versatile for diverse datasets. They excel in multilingual scenarios while supporting multimodal input through late fusion strategies that combine modality-specific embeddings at later layers, thus preserving modality-specific features alongside integrated representations ([Source](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\n- **Open-Source Models: Qwen2.5-VL-32B-Instruct**  \\n  Open-source innovation continues with models like Qwen2.5-VL-32B-Instruct, which emphasize **visual reasoning and structured data extraction**. This model uses a multi-stage fusion technique, combining visual token embeddings and text tokens with task-specific heads focused on structured outputs such as tables and object attributes. Its open availability extends capabilities to broader developer communities and accelerates application development in sectors needing visual data interpretation ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\n### Architectural Comparison\\n\\n- **Mixture-of-Experts (MoE):**  \\n  Exemplified by Llama 4, MoE provides computational efficiency by activating only a subset of network “experts” per input, enabling scaling without linear compute growth.\\n\\n- **Transformer-Based Backbones with Cross-Modal Attention:**  \\n  Used in Gemini 3 Pro, transformers with integrated cross-modal attention layers enable deep contextual interactions between modalities, improving reasoning and coherence.\\n\\n- **Late Fusion Techniques:**  \\n  Falcon models apply fusion at later layers, effectively maintaining modality-specific nuances while enabling joint representation learning.\\n\\n- **Multi-Stage Fusion for Structured Reasoning:**  \\n  Seen in Qwen2.5-VL, this method strategically stages combination of modality data and adds instruction-tuned heads, enhancing capabilities for complex specialized tasks.\\n\\nThese architectural trends illustrate the balance between maximizing multimodal synergy and maintaining efficiency. Developers aiming to build or integrate multimodal LLMs in 2026 will find it crucial to consider how these design philosophies affect model interpretability, latency, and scalability.  \\n\\n[Sources: Shakudo, Clarifai, TechTarget, SiliconFlow]'),\n",
       "  (2,\n",
       "   \"## Expanding Modalities: Text, Vision, Audio, and Video\\n\\nIn 2026, leading multimodal large language models (LLMs) like Google's Gemini and Meta's LLaMA 4 have significantly expanded their native ability to process multiple input modalities beyond text, including images, audio, and video. This multimodal support enables richer, more natural interactions and broader application scenarios compared to earlier generations focused primarily on text ([Shakudo](https://www.shakudo.io/blog/top-9-large-language-models); [Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n### Native Support for Diverse Modalities\\n\\nBoth Gemini and LLaMA 4 can natively ingest and interpret multimodal data streams. Text remains foundational, but integrated image processing now allows detailed scene understanding, object recognition, and diagram interpretation. Audio capabilities include speech recognition combined with prosody and emotion analysis, while video inputs are parsed frame-by-frame for dynamic content analysis such as motion tracking and event detection. This comprehensive approach markedly improves the context and nuance in model responses ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\\n\\n### Audio and Real-Time Voice Interaction\\n\\nA key advancement is the integration of audio and real-time voice modes, which enables these models to analyze tone, accent, and mood dynamically during interaction. This allows adaptive conversational style adjustments—influencing politeness, formality, or urgency based on user cues. As a result, applications like virtual assistants and interactive voice response (IVR) systems achieve far more natural and contextually appropriate engagement ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\n### Advances in Video and Image Understanding\\n\\nVideo processing capabilities enable temporal analysis such as activity recognition and event segmentation across video streams. Improvements in image understanding support precise object localization, semantic segmentation, and even complex tasks like interpreting scientific diagrams and engineering blueprints. These functions greatly aid areas like automated video annotation, surveillance analysis, and educational content generation ([TechTarget](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\n### Challenges and Limitations\\n\\nDespite progress, modality integration remains a challenge. Hallucination—generating inaccurate or fabricated details—occurs when combining noisy visual or audio inputs with ambiguous text context. Synchronizing temporal and semantic information across modalities like video and audio also requires ongoing refinement to avoid misaligned or inconsistent outputs. Current models continue to improve robustness and alignment through enhanced training techniques and multimodal attention architectures ([AIMultiple](https://research.aimultiple.com/large-multimodal-models/)).\\n\\n### Industry Applications\\n\\nThe expanded multimodal capabilities have unlocked diverse applications:\\n\\n- **Multimedia Search:** Searching databases using combined text, image, and video queries allows intuitive retrieval of complex content such as product images matching textual descriptions.\\n- **Scientific Analysis:** Processing scientific papers containing text, charts, and images enables automated summarization and insight extraction.\\n- **Design Generation:** Creative industries leverage these models to generate graphic designs, video storyboards, or audio compositions guided by multimodal prompts.\\n\\nIn summary, 2026’s multimodal LLMs represent a pivotal step toward fully integrated AI systems capable of understanding and generating across text, vision, audio, and video, powering innovative applications across technology and industry sectors ([ZenMux](https://zenmux.ai/blog/top-10-most-popular-llms-in-2026)).\"),\n",
       "  (3,\n",
       "   \"## Context Window Expansion and Memory Enhancements\\n\\nIn 2026, one of the most remarkable advancements in multimodal large language models (LLMs) is the dramatic increase in context window sizes. Leading models like Gemini 3 Pro now support context lengths of up to 2 million tokens, a significant leap from the hundreds of thousands of tokens available just a few years ago. This expanded context enables the model to ingest entire knowledge bases, massive code repositories, and lengthy documents all at once, facilitating more comprehensive analysis and synthesis in a single inference pass ([Shakudo](https://www.shakudo.io/blog/top-9-large-language-models), [ZenMux](https://zenmux.ai/blog/top-10-most-popular-llms-in-2026)).\\n\\nExtended context windows allow multimodal LLMs to maintain awareness across vast and complex inputs, which is particularly valuable for developers aiming to create intelligent assistants capable of understanding nuanced, long-form content or cross-referencing detailed data points. For example, software development tools can now analyze the full scope of a large codebase without needing to chunk input and lose inter-file context, supporting more precise code completions, bug detection, and refactoring suggestions ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nAlongside context length, advances in memory systems have significantly improved LLMs' ability to recall earlier parts of a conversation or earlier input elements throughout extended interactions. Persistent memory layers and optimized retrieval mechanisms enable models to sustain contextual awareness over multiple sessions or prolonged exchanges. This feature is critical for building applications like contextually aware chatbots, personalized assistants, and analytic dashboards which rely on remembering user preferences or previous insights to provide continuity and enhance user experience ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\\n\\nHowever, these benefits come with trade-offs. Very large context windows substantially increase computational and memory demands, leading to higher inference latencies and operational costs. Developers must balance the need for extensive context with performance considerations, particularly for real-time or resource-constrained applications. Techniques like adaptive context compression, hierarchical memory architectures, and specialized hardware accelerators are emerging to help mitigate these challenges and optimize throughput ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nIn summary, context window expansion and memory enhancements in 2026 multimodal LLMs unlock new possibilities for complex reasoning and long-term contextual understanding. These capabilities empower developers to build more contextually rich, intelligent tools, albeit with mindful attention to the increased resource requirements involved.\"),\n",
       "  (4,\n",
       "   '## Open-Source Multimodal Models and Democratization of AI\\n\\nKey open-source multimodal LLMs shaping AI accessibility in 2026 include the Falcon series and Qwen2.5-VL-32B-Instruct. The Falcon models are notable for their open governance and strong text-image understanding capabilities, while Qwen2.5-VL-32B-Instruct stands out with advanced visual reasoning and interactive tool use, combining instruction following with rich multimodal inputs ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nOpen-source licensing has been instrumental in democratizing AI research and deployment. By providing free and transparent access to these powerful models, the community can experiment with novel architectures, adapt models to niche domains, and integrate LLMs across diverse applications—from healthcare diagnostics to creative content generation. This openness accelerates innovation cycles and lowers barriers to entry for startups and academic groups ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nPerformance-wise, open-source models like Qwen2.5-VL-32B-Instruct offer noteworthy benefits. Its ability to reason visually allows improved interpretation of images combined with text, enabling sophisticated multi-turn dialogues involving complex visual content. Moreover, optimizations in model architecture translate into faster inference times compared to many proprietary counterparts, making them practical for real-time, interactive systems ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nThe broader ecosystem around these models thrives on vibrant community contributions. Forking repositories, fine-tuning on specialized datasets, and optimizing for edge or cloud deployment have created a rich landscape of derivative models tailored to specific use cases. This collaborative environment fosters modularity and interoperability, enabling developers to compose multimodal AI solutions with considerable flexibility ([Source](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\nHowever, open-source multimodal LLMs also face challenges. Maintaining compatibility across rapidly evolving codebases demands significant effort, while security vulnerabilities in open code can expose applications to risks if not promptly addressed. Additionally, as models proliferate, fragmentation and convergence issues emerge, complicating benchmarking and standardization efforts essential for robust, reproducible AI systems ([Source](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\nIn summary, open-source multimodal LLMs such as Falcon and Qwen2.5-VL-32B-Instruct are critical drivers in broadening AI access and fostering innovation. The trade-offs of increased maintenance and security complexity are balanced by the dynamic, community-powered ecosystem enabling diverse, high-performance AI capabilities.'),\n",
       "  (5,\n",
       "   '## Advancements in Reasoning and Agentic Capabilities\\n\\nIn 2026, multimodal large language models (LLMs) have achieved significant advancements in both reasoning complexity and autonomous decision-making, empowering developers with more strategic and context-aware AI systems.\\n\\nOne standout feature is the ‘Deep Think’ mode introduced in Google’s Gemini 3 Pro. This mode enables the model to engage in parallel, strategic reasoning processes, allowing it to solve complex and multi-faceted problems more efficiently than prior single-threaded approaches. By simulating simultaneous thought streams, Gemini 3 Pro can evaluate multiple hypotheses and plan intricate solutions, markedly enhancing problem-solving depth ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nComplementing this, Qwen 2.5 employs advanced reinforcement learning strategies specifically targeted at boosting mathematical and logical capabilities. Through iterative feedback mechanisms and reward-driven optimization, Qwen 2.5 continuously improves its precision in complex calculations and abstract problem-solving tasks. This reinforcement framework ensures that its mathematical reasoning evolves with exposure to diverse problem classes, outperforming earlier purely supervised models ([Source](https://zenmux.ai/blog/top-10-most-popular-llms-in-2026)).\\n\\nAgentic capabilities are now a defining characteristic of state-of-the-art multimodal LLMs. These models can dynamically select and direct external tools—ranging from code repositories to data visualization utilities—to perform multi-step tasks autonomously within workflows. This autonomy allows a single model query to trigger actions such as fetching data, running analyses, and synthesizing results without requiring manual intermediate input, vastly streamlining complex workflows and elevating productivity ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nCrucially, improvements in reasoning have been accompanied by advancements in factual accuracy and hallucination control, especially in image and diagram understanding contexts. Enhanced training datasets and cross-modal alignment techniques reduce spurious outputs, ensuring that models interpret and integrate visual information reliably. This has been vital in domains where misinterpretation of charts or diagrams could lead to incorrect conclusions ([Source](https://research.aimultiple.com/large-multimodal-models/)).\\n\\nThese enhancements enable a wide array of impactful use cases:\\n- **Scientific analysis:** Models can dissect experimental data, propose hypotheses, and validate results across textual and visual modalities.\\n- **Software development assistants:** Developers benefit from agents that can understand design diagrams, generate code, and debug autonomously.\\n- **Enterprise search:** Multimodal querying combines document text with embedded images and charts to deliver richer, context-aware search results.\\n\\nTogether, the deepened reasoning and agentic abilities of 2026’s multimodal LLMs signify a major leap toward AI systems that think strategically, act independently, and handle rich, multifaceted inputs with high reliability.'),\n",
       "  (6,\n",
       "   \"## Performance, Efficiency, and Architectural Trade-offs\\n\\nIn 2026, multimodal large language models (LLMs) have advanced significantly, balancing the need for high performance with manageable computational costs. One of the pivotal architectural innovations enabling this balance is the widespread adoption of Mixture-of-Experts (MoE) frameworks, notably exemplified in the Llama 4 series. MoE architectures dynamically route inputs to a subset of specialized expert subnetworks during inference, drastically reducing the amount of active computation while maintaining model capacity. This selective activation approach allows Llama 4 models to offer competitive inference speeds and resource efficiency, making them attractive for both research and enterprise applications ([Shakudo](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\nWhen comparing open-source versus proprietary multimodal LLMs, significant differences in hardware requirements and throughput remain. Open-source models prioritize accessibility and flexibility, often optimizing for deployment on commodity GPUs with mixed precision and efficient attention mechanisms. Proprietary models, conversely, leverage custom hardware accelerators and aggressive model pruning or quantization to achieve faster inference speeds. SiliconFlow's benchmarking highlights that while proprietary offerings generally lead in raw throughput, leading open-source multimodal models have closed the gap substantially thanks to innovations like sparse activation and streamlined token aggregation ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nTraining pipelines for multimodal LLMs have evolved to incorporate complex stages beyond typical pretraining. Supervised fine-tuning remains essential to align the model with specific multimodal tasks, such as image captioning or video summarization. Reinforcement learning techniques, particularly Reinforcement Learning with Human Feedback (RLHF), refine output quality by optimizing for user preferences and contextual relevance. More recently, direct preference optimization methods allow models to adjust behavior in a single-stage fine-tuning procedure, reducing training complexity while improving alignment across text, vision, and other modalities ([Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nFrom a cost perspective, running very large multimodal LLMs presents substantial enterprise challenges. These models demand extensive GPU clusters with fast interconnects to handle their scale and bandwidth requirements. The incremental cost of inference, including energy consumption and hardware depreciation, necessitates optimization strategies such as model distillation or runtime adaptation to reduce overhead. Scalability is also impacted by data pipeline complexity and cloud infrastructure costs, making model selection and size tuning crucial decisions for production environments ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\\n\\nPerformance benchmarking for multimodal LLMs in 2026 incorporates not only traditional text generation metrics like perplexity and BLEU but also multimodal-specific metrics. These include image-text retrieval accuracy, video question answering scores, and cross-modal reasoning benchmarks. Current benchmarks stress real-world end-to-end tasks to evaluate joint understanding and generation across modalities, reflecting the diverse usage scenarios now supported by leading models ([AIMultiple](https://research.aimultiple.com/large-multimodal-models/)).\\n\\nIn summary, the state of multimodal LLMs in 2026 hinges on architectural designs like MoE for efficiency, sophisticated training regimes that integrate preference tuning, and careful consideration of cost versus performance trade-offs for scalable deployment. Benchmarking has matured to assess multi-aspect capabilities essential for practical multimodal AI systems.\"),\n",
       "  (7,\n",
       "   '## Applications and Industry Impact of Multimodal LLMs in 2026\\n\\nMultimodal large language models (LLMs) have become pivotal in transforming multiple industries by enabling applications that integrate text, images, audio, and video inputs for richer AI understanding and interaction. In scientific research, these models assist with complex data interpretation by combining textual analysis with image and graph recognition, accelerating insights in fields like genomics and climate science. Multimedia search engines now leverage multimodal LLMs to index and retrieve results by understanding context across formats, improving the user experience beyond keyword matching. In software development, multimodal LLMs contribute to code generation and debugging by analyzing documentation, code snippets, and even visual diagrams simultaneously. Design and creative industries utilize these models to generate novel visual assets, interactive prototypes, and concept art from descriptive text inputs, effectively augmenting creative workflows ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nReal-time video understanding is another crucial application, enabling surveillance, sports analytics, and live content moderation by interpreting video streams alongside associated audio and text metadata. Enterprise search solutions use multimodal LLMs to unify information retrieval across documents, images, and emails, enhancing knowledge management. Interactive agents powered by multimodal models provide more natural and contextual assistance by recognizing user inputs that combine speech, gestures, and visual cues, improving customer support and virtual collaboration platforms ([Source](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\nThese advances are instrumental in democratizing AI by making sophisticated multimodal capabilities accessible to a broader user base. Specialized multimodal models address domain-specific challenges, delivering tailored solutions in healthcare diagnostics, legal analysis, and manufacturing process optimization where integrating diverse data types is critical. This has significantly lowered barriers for non-expert users and small businesses to deploy advanced AI tools within their workflows ([Source](https://research.aimultiple.com/large-multimodal-models/)).\\n\\nThe broader societal and business impacts include increased automation of routine knowledge work, such as report generation, data summarization, and content creation, freeing professionals to focus on strategic tasks. Augmentation of knowledge work has improved decision-making quality by synthesizing multimodal information streams in real time. Accessibility enhancements are prominent as multimodal LLMs facilitate real-time captioning, sign language interpretation, and personalized learning aids, substantially benefiting users with disabilities ([Source](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\nEmerging fields harnessing multimodal LLM capabilities include augmented reality (AR), where these models enable more intuitive environment understanding and context-aware interactions within AR experiences. Robotics interfaces increasingly integrate multimodal models to interpret complex sensor inputs, voice commands, and visual feedback, allowing robots to operate more flexibly in dynamic real-world settings. These applications signal a convergence of AI modalities that is reshaping how digital and physical systems collaborate ([Source](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms)).\\n\\nIn summary, multimodal LLMs in 2026 are driving innovative applications across industries, enabling richer data comprehension, improving user interaction paradigms, and fostering new AI-driven business models with significant societal benefits.')],\n",
       " 'final': \"# State of Multimodal LLMs in 2026: Overview and Industry Implications\\n\\n## Overview of Leading Multimodal LLM Architectures in 2026\\n\\nIn 2026, the landscape of multimodal large language models (LLMs) is defined by diverse architectural innovations aimed at enhancing efficiency, multimodal understanding, and task generalization. Here’s a snapshot of the foremost models and their key architectural traits:\\n\\n- **Meta’s Llama 4 Series (Scout and Maverick)**  \\n  Meta’s Llama 4 variants leverage a **Mixture-of-Experts (MoE)** architecture, where different expert subnetworks are dynamically activated per input. This approach significantly improves computation efficiency and scalability while maintaining performance. Scout and Maverick specialize in balancing general language understanding with refined multimodal capabilities, enabling them to process complex text and visual inputs simultaneously without excessive resource consumption ([Source](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\n- **Google’s Gemini Series**  \\n  Gemini models, particularly the **Gemini 3 Pro**, integrate deep multimodal processing layers with advanced agentic capabilities, allowing them to perform autonomous reasoning and decision-making across text, images, and other modalities. Their architecture builds on a high-capacity Transformer backbone fused with specialized cross-modal attention mechanisms that tightly connect visual and textual streams. This fusion facilitates nuanced contextual understanding beyond simple feature concatenation ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n- **Falcon Models from Technology Innovation Institute**  \\n  The Falcon family uses a robust **transformer-centric design** tuned for multilingual and multimodal data. Emphasis on large-scale text and image tokenization techniques makes Falcon models versatile for diverse datasets. They excel in multilingual scenarios while supporting multimodal input through late fusion strategies that combine modality-specific embeddings at later layers, thus preserving modality-specific features alongside integrated representations ([Source](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\n- **Open-Source Models: Qwen2.5-VL-32B-Instruct**  \\n  Open-source innovation continues with models like Qwen2.5-VL-32B-Instruct, which emphasize **visual reasoning and structured data extraction**. This model uses a multi-stage fusion technique, combining visual token embeddings and text tokens with task-specific heads focused on structured outputs such as tables and object attributes. Its open availability extends capabilities to broader developer communities and accelerates application development in sectors needing visual data interpretation ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\n### Architectural Comparison\\n\\n- **Mixture-of-Experts (MoE):**  \\n  Exemplified by Llama 4, MoE provides computational efficiency by activating only a subset of network “experts” per input, enabling scaling without linear compute growth.\\n\\n- **Transformer-Based Backbones with Cross-Modal Attention:**  \\n  Used in Gemini 3 Pro, transformers with integrated cross-modal attention layers enable deep contextual interactions between modalities, improving reasoning and coherence.\\n\\n- **Late Fusion Techniques:**  \\n  Falcon models apply fusion at later layers, effectively maintaining modality-specific nuances while enabling joint representation learning.\\n\\n- **Multi-Stage Fusion for Structured Reasoning:**  \\n  Seen in Qwen2.5-VL, this method strategically stages combination of modality data and adds instruction-tuned heads, enhancing capabilities for complex specialized tasks.\\n\\nThese architectural trends illustrate the balance between maximizing multimodal synergy and maintaining efficiency. Developers aiming to build or integrate multimodal LLMs in 2026 will find it crucial to consider how these design philosophies affect model interpretability, latency, and scalability.  \\n\\n[Sources: Shakudo, Clarifai, TechTarget, SiliconFlow]\\n\\n## Expanding Modalities: Text, Vision, Audio, and Video\\n\\nIn 2026, leading multimodal large language models (LLMs) like Google's Gemini and Meta's LLaMA 4 have significantly expanded their native ability to process multiple input modalities beyond text, including images, audio, and video. This multimodal support enables richer, more natural interactions and broader application scenarios compared to earlier generations focused primarily on text ([Shakudo](https://www.shakudo.io/blog/top-9-large-language-models); [Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\n### Native Support for Diverse Modalities\\n\\nBoth Gemini and LLaMA 4 can natively ingest and interpret multimodal data streams. Text remains foundational, but integrated image processing now allows detailed scene understanding, object recognition, and diagram interpretation. Audio capabilities include speech recognition combined with prosody and emotion analysis, while video inputs are parsed frame-by-frame for dynamic content analysis such as motion tracking and event detection. This comprehensive approach markedly improves the context and nuance in model responses ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\\n\\n### Audio and Real-Time Voice Interaction\\n\\nA key advancement is the integration of audio and real-time voice modes, which enables these models to analyze tone, accent, and mood dynamically during interaction. This allows adaptive conversational style adjustments—influencing politeness, formality, or urgency based on user cues. As a result, applications like virtual assistants and interactive voice response (IVR) systems achieve far more natural and contextually appropriate engagement ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\n### Advances in Video and Image Understanding\\n\\nVideo processing capabilities enable temporal analysis such as activity recognition and event segmentation across video streams. Improvements in image understanding support precise object localization, semantic segmentation, and even complex tasks like interpreting scientific diagrams and engineering blueprints. These functions greatly aid areas like automated video annotation, surveillance analysis, and educational content generation ([TechTarget](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\n### Challenges and Limitations\\n\\nDespite progress, modality integration remains a challenge. Hallucination—generating inaccurate or fabricated details—occurs when combining noisy visual or audio inputs with ambiguous text context. Synchronizing temporal and semantic information across modalities like video and audio also requires ongoing refinement to avoid misaligned or inconsistent outputs. Current models continue to improve robustness and alignment through enhanced training techniques and multimodal attention architectures ([AIMultiple](https://research.aimultiple.com/large-multimodal-models/)).\\n\\n### Industry Applications\\n\\nThe expanded multimodal capabilities have unlocked diverse applications:\\n\\n- **Multimedia Search:** Searching databases using combined text, image, and video queries allows intuitive retrieval of complex content such as product images matching textual descriptions.\\n- **Scientific Analysis:** Processing scientific papers containing text, charts, and images enables automated summarization and insight extraction.\\n- **Design Generation:** Creative industries leverage these models to generate graphic designs, video storyboards, or audio compositions guided by multimodal prompts.\\n\\nIn summary, 2026’s multimodal LLMs represent a pivotal step toward fully integrated AI systems capable of understanding and generating across text, vision, audio, and video, powering innovative applications across technology and industry sectors ([ZenMux](https://zenmux.ai/blog/top-10-most-popular-llms-in-2026)).\\n\\n## Context Window Expansion and Memory Enhancements\\n\\nIn 2026, one of the most remarkable advancements in multimodal large language models (LLMs) is the dramatic increase in context window sizes. Leading models like Gemini 3 Pro now support context lengths of up to 2 million tokens, a significant leap from the hundreds of thousands of tokens available just a few years ago. This expanded context enables the model to ingest entire knowledge bases, massive code repositories, and lengthy documents all at once, facilitating more comprehensive analysis and synthesis in a single inference pass ([Shakudo](https://www.shakudo.io/blog/top-9-large-language-models), [ZenMux](https://zenmux.ai/blog/top-10-most-popular-llms-in-2026)).\\n\\nExtended context windows allow multimodal LLMs to maintain awareness across vast and complex inputs, which is particularly valuable for developers aiming to create intelligent assistants capable of understanding nuanced, long-form content or cross-referencing detailed data points. For example, software development tools can now analyze the full scope of a large codebase without needing to chunk input and lose inter-file context, supporting more precise code completions, bug detection, and refactoring suggestions ([Clarifai Industry Guide](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nAlongside context length, advances in memory systems have significantly improved LLMs' ability to recall earlier parts of a conversation or earlier input elements throughout extended interactions. Persistent memory layers and optimized retrieval mechanisms enable models to sustain contextual awareness over multiple sessions or prolonged exchanges. This feature is critical for building applications like contextually aware chatbots, personalized assistants, and analytic dashboards which rely on remembering user preferences or previous insights to provide continuity and enhance user experience ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\\n\\nHowever, these benefits come with trade-offs. Very large context windows substantially increase computational and memory demands, leading to higher inference latencies and operational costs. Developers must balance the need for extensive context with performance considerations, particularly for real-time or resource-constrained applications. Techniques like adaptive context compression, hierarchical memory architectures, and specialized hardware accelerators are emerging to help mitigate these challenges and optimize throughput ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nIn summary, context window expansion and memory enhancements in 2026 multimodal LLMs unlock new possibilities for complex reasoning and long-term contextual understanding. These capabilities empower developers to build more contextually rich, intelligent tools, albeit with mindful attention to the increased resource requirements involved.\\n\\n## Open-Source Multimodal Models and Democratization of AI\\n\\nKey open-source multimodal LLMs shaping AI accessibility in 2026 include the Falcon series and Qwen2.5-VL-32B-Instruct. The Falcon models are notable for their open governance and strong text-image understanding capabilities, while Qwen2.5-VL-32B-Instruct stands out with advanced visual reasoning and interactive tool use, combining instruction following with rich multimodal inputs ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nOpen-source licensing has been instrumental in democratizing AI research and deployment. By providing free and transparent access to these powerful models, the community can experiment with novel architectures, adapt models to niche domains, and integrate LLMs across diverse applications—from healthcare diagnostics to creative content generation. This openness accelerates innovation cycles and lowers barriers to entry for startups and academic groups ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nPerformance-wise, open-source models like Qwen2.5-VL-32B-Instruct offer noteworthy benefits. Its ability to reason visually allows improved interpretation of images combined with text, enabling sophisticated multi-turn dialogues involving complex visual content. Moreover, optimizations in model architecture translate into faster inference times compared to many proprietary counterparts, making them practical for real-time, interactive systems ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nThe broader ecosystem around these models thrives on vibrant community contributions. Forking repositories, fine-tuning on specialized datasets, and optimizing for edge or cloud deployment have created a rich landscape of derivative models tailored to specific use cases. This collaborative environment fosters modularity and interoperability, enabling developers to compose multimodal AI solutions with considerable flexibility ([Source](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\nHowever, open-source multimodal LLMs also face challenges. Maintaining compatibility across rapidly evolving codebases demands significant effort, while security vulnerabilities in open code can expose applications to risks if not promptly addressed. Additionally, as models proliferate, fragmentation and convergence issues emerge, complicating benchmarking and standardization efforts essential for robust, reproducible AI systems ([Source](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\nIn summary, open-source multimodal LLMs such as Falcon and Qwen2.5-VL-32B-Instruct are critical drivers in broadening AI access and fostering innovation. The trade-offs of increased maintenance and security complexity are balanced by the dynamic, community-powered ecosystem enabling diverse, high-performance AI capabilities.\\n\\n## Advancements in Reasoning and Agentic Capabilities\\n\\nIn 2026, multimodal large language models (LLMs) have achieved significant advancements in both reasoning complexity and autonomous decision-making, empowering developers with more strategic and context-aware AI systems.\\n\\nOne standout feature is the ‘Deep Think’ mode introduced in Google’s Gemini 3 Pro. This mode enables the model to engage in parallel, strategic reasoning processes, allowing it to solve complex and multi-faceted problems more efficiently than prior single-threaded approaches. By simulating simultaneous thought streams, Gemini 3 Pro can evaluate multiple hypotheses and plan intricate solutions, markedly enhancing problem-solving depth ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nComplementing this, Qwen 2.5 employs advanced reinforcement learning strategies specifically targeted at boosting mathematical and logical capabilities. Through iterative feedback mechanisms and reward-driven optimization, Qwen 2.5 continuously improves its precision in complex calculations and abstract problem-solving tasks. This reinforcement framework ensures that its mathematical reasoning evolves with exposure to diverse problem classes, outperforming earlier purely supervised models ([Source](https://zenmux.ai/blog/top-10-most-popular-llms-in-2026)).\\n\\nAgentic capabilities are now a defining characteristic of state-of-the-art multimodal LLMs. These models can dynamically select and direct external tools—ranging from code repositories to data visualization utilities—to perform multi-step tasks autonomously within workflows. This autonomy allows a single model query to trigger actions such as fetching data, running analyses, and synthesizing results without requiring manual intermediate input, vastly streamlining complex workflows and elevating productivity ([Source](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nCrucially, improvements in reasoning have been accompanied by advancements in factual accuracy and hallucination control, especially in image and diagram understanding contexts. Enhanced training datasets and cross-modal alignment techniques reduce spurious outputs, ensuring that models interpret and integrate visual information reliably. This has been vital in domains where misinterpretation of charts or diagrams could lead to incorrect conclusions ([Source](https://research.aimultiple.com/large-multimodal-models/)).\\n\\nThese enhancements enable a wide array of impactful use cases:\\n- **Scientific analysis:** Models can dissect experimental data, propose hypotheses, and validate results across textual and visual modalities.\\n- **Software development assistants:** Developers benefit from agents that can understand design diagrams, generate code, and debug autonomously.\\n- **Enterprise search:** Multimodal querying combines document text with embedded images and charts to deliver richer, context-aware search results.\\n\\nTogether, the deepened reasoning and agentic abilities of 2026’s multimodal LLMs signify a major leap toward AI systems that think strategically, act independently, and handle rich, multifaceted inputs with high reliability.\\n\\n## Performance, Efficiency, and Architectural Trade-offs\\n\\nIn 2026, multimodal large language models (LLMs) have advanced significantly, balancing the need for high performance with manageable computational costs. One of the pivotal architectural innovations enabling this balance is the widespread adoption of Mixture-of-Experts (MoE) frameworks, notably exemplified in the Llama 4 series. MoE architectures dynamically route inputs to a subset of specialized expert subnetworks during inference, drastically reducing the amount of active computation while maintaining model capacity. This selective activation approach allows Llama 4 models to offer competitive inference speeds and resource efficiency, making them attractive for both research and enterprise applications ([Shakudo](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\nWhen comparing open-source versus proprietary multimodal LLMs, significant differences in hardware requirements and throughput remain. Open-source models prioritize accessibility and flexibility, often optimizing for deployment on commodity GPUs with mixed precision and efficient attention mechanisms. Proprietary models, conversely, leverage custom hardware accelerators and aggressive model pruning or quantization to achieve faster inference speeds. SiliconFlow's benchmarking highlights that while proprietary offerings generally lead in raw throughput, leading open-source multimodal models have closed the gap substantially thanks to innovations like sparse activation and streamlined token aggregation ([SiliconFlow](https://www.siliconflow.com/articles/en/fastest-open-source-multimodal-models)).\\n\\nTraining pipelines for multimodal LLMs have evolved to incorporate complex stages beyond typical pretraining. Supervised fine-tuning remains essential to align the model with specific multimodal tasks, such as image captioning or video summarization. Reinforcement learning techniques, particularly Reinforcement Learning with Human Feedback (RLHF), refine output quality by optimizing for user preferences and contextual relevance. More recently, direct preference optimization methods allow models to adjust behavior in a single-stage fine-tuning procedure, reducing training complexity while improving alignment across text, vision, and other modalities ([Clarifai](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nFrom a cost perspective, running very large multimodal LLMs presents substantial enterprise challenges. These models demand extensive GPU clusters with fast interconnects to handle their scale and bandwidth requirements. The incremental cost of inference, including energy consumption and hardware depreciation, necessitates optimization strategies such as model distillation or runtime adaptation to reduce overhead. Scalability is also impacted by data pipeline complexity and cloud infrastructure costs, making model selection and size tuning crucial decisions for production environments ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms/)).\\n\\nPerformance benchmarking for multimodal LLMs in 2026 incorporates not only traditional text generation metrics like perplexity and BLEU but also multimodal-specific metrics. These include image-text retrieval accuracy, video question answering scores, and cross-modal reasoning benchmarks. Current benchmarks stress real-world end-to-end tasks to evaluate joint understanding and generation across modalities, reflecting the diverse usage scenarios now supported by leading models ([AIMultiple](https://research.aimultiple.com/large-multimodal-models/)).\\n\\nIn summary, the state of multimodal LLMs in 2026 hinges on architectural designs like MoE for efficiency, sophisticated training regimes that integrate preference tuning, and careful consideration of cost versus performance trade-offs for scalable deployment. Benchmarking has matured to assess multi-aspect capabilities essential for practical multimodal AI systems.\\n\\n## Applications and Industry Impact of Multimodal LLMs in 2026\\n\\nMultimodal large language models (LLMs) have become pivotal in transforming multiple industries by enabling applications that integrate text, images, audio, and video inputs for richer AI understanding and interaction. In scientific research, these models assist with complex data interpretation by combining textual analysis with image and graph recognition, accelerating insights in fields like genomics and climate science. Multimedia search engines now leverage multimodal LLMs to index and retrieve results by understanding context across formats, improving the user experience beyond keyword matching. In software development, multimodal LLMs contribute to code generation and debugging by analyzing documentation, code snippets, and even visual diagrams simultaneously. Design and creative industries utilize these models to generate novel visual assets, interactive prototypes, and concept art from descriptive text inputs, effectively augmenting creative workflows ([Source](https://www.clarifai.com/blog/llms-and-ai-trends)).\\n\\nReal-time video understanding is another crucial application, enabling surveillance, sports analytics, and live content moderation by interpreting video streams alongside associated audio and text metadata. Enterprise search solutions use multimodal LLMs to unify information retrieval across documents, images, and emails, enhancing knowledge management. Interactive agents powered by multimodal models provide more natural and contextual assistance by recognizing user inputs that combine speech, gestures, and visual cues, improving customer support and virtual collaboration platforms ([Source](https://www.techtarget.com/whatis/feature/12-of-the-best-large-language-models)).\\n\\nThese advances are instrumental in democratizing AI by making sophisticated multimodal capabilities accessible to a broader user base. Specialized multimodal models address domain-specific challenges, delivering tailored solutions in healthcare diagnostics, legal analysis, and manufacturing process optimization where integrating diverse data types is critical. This has significantly lowered barriers for non-expert users and small businesses to deploy advanced AI tools within their workflows ([Source](https://research.aimultiple.com/large-multimodal-models/)).\\n\\nThe broader societal and business impacts include increased automation of routine knowledge work, such as report generation, data summarization, and content creation, freeing professionals to focus on strategic tasks. Augmentation of knowledge work has improved decision-making quality by synthesizing multimodal information streams in real time. Accessibility enhancements are prominent as multimodal LLMs facilitate real-time captioning, sign language interpretation, and personalized learning aids, substantially benefiting users with disabilities ([Source](https://www.shakudo.io/blog/top-9-large-language-models)).\\n\\nEmerging fields harnessing multimodal LLM capabilities include augmented reality (AR), where these models enable more intuitive environment understanding and context-aware interactions within AR experiences. Robotics interfaces increasingly integrate multimodal models to interpret complex sensor inputs, voice commands, and visual feedback, allowing robots to operate more flexibly in dynamic real-world settings. These applications signal a convergence of AI modalities that is reshaping how digital and physical systems collaborate ([Source](https://www.analyticsvidhya.com/blog/2025/03/top-multimodal-llms)).\\n\\nIn summary, multimodal LLMs in 2026 are driving innovative applications across industries, enabling richer data comprehension, improving user interaction paradigms, and fostering new AI-driven business models with significant societal benefits.\\n\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run(\"Write a blog on Open Source LLMs in 2026\")\n",
    "run(\"State of Multimodal LLMs in 2026\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c050418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
