{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476ded0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb4e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # keep if Tavily provides; DO NOT rely on it\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ImageSpec(BaseModel):\n",
    "    placeholder: str = Field(..., description=\"e.g. [[IMAGE_1]]\")\n",
    "    filename: str = Field(..., description=\"Save under images/, e.g. qkv_flow.png\")\n",
    "    alt: str\n",
    "    caption: str\n",
    "    prompt: str = Field(..., description=\"Prompt to send to the image model.\")\n",
    "    size: Literal[\"1024x1024\", \"1024x1536\", \"1536x1024\"] = \"1024x1024\"\n",
    "    quality: Literal[\"low\", \"medium\", \"high\"] = \"medium\"\n",
    "\n",
    "\n",
    "class GlobalImagePlan(BaseModel):\n",
    "    md_with_placeholders: str\n",
    "    images: List[ImageSpec] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f8957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "\n",
    "    # reducer/image\n",
    "    merged_md: str\n",
    "    md_with_placeholders: str\n",
    "    image_specs: List[dict]\n",
    "\n",
    "    final: str\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1c0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- If user asked for \"last week/this week/latest\", reflect that constraint IN THE QUERIES.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "    \n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09e9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily) \n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- If a published date is explicitly present in the result payload, keep it as YYYY-MM-DD.\n",
    "  If missing or unclear, set published_at=null. Do NOT guess.\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "\n",
    "    # take the first 10 queries from state\n",
    "    queries = (state.get(\"queries\", []) or [])\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(content=f\"Raw results:\\n{raw_results}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "\n",
    "    return {\"evidence\": list(dedup.values())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6c91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book:\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f634ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard:\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book:\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true:\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [(task.id, section_md)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4856b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) ReducerWithImages (subgraph)\n",
    "#    merge_content -> decide_images -> generate_and_place_images\n",
    "# ============================================================\n",
    "def merge_content(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    merged_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "    return {\"merged_md\": merged_md}\n",
    "\n",
    "\n",
    "DECIDE_IMAGES_SYSTEM = \"\"\"You are an expert technical editor.\n",
    "Decide if images/diagrams are needed for THIS blog.\n",
    "\n",
    "Rules:\n",
    "- Max 3 images total.\n",
    "- Each image must materially improve understanding (diagram/flow/table-like visual).\n",
    "- Insert placeholders exactly: [[IMAGE_1]], [[IMAGE_2]], [[IMAGE_3]].\n",
    "- If no images needed: md_with_placeholders must equal input and images=[].\n",
    "- Avoid decorative images; prefer technical diagrams with short labels.\n",
    "Return strictly GlobalImagePlan.\n",
    "\"\"\"\n",
    "\n",
    "def decide_images(state: State) -> dict:\n",
    "    \n",
    "    planner = llm.with_structured_output(GlobalImagePlan)\n",
    "    merged_md = state[\"merged_md\"]\n",
    "    plan = state[\"plan\"]\n",
    "    assert plan is not None\n",
    "\n",
    "    image_plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=DECIDE_IMAGES_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Topic: {state['topic']}\\n\\n\"\n",
    "                    \"Insert placeholders + propose image prompts.\\n\\n\"\n",
    "                    f\"{merged_md}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"md_with_placeholders\": image_plan.md_with_placeholders,\n",
    "        \"image_specs\": [img.model_dump() for img in image_plan.images],\n",
    "    }\n",
    "\n",
    "\n",
    "def _gemini_generate_image_bytes(prompt: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Returns raw image bytes generated by Gemini.\n",
    "    Requires: pip install google-genai\n",
    "    Env var: GOOGLE_API_KEY\n",
    "    \"\"\"\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "\n",
    "    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"GOOGLE_API_KEY is not set.\")\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-image\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            response_modalities=[\"IMAGE\"],\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                    threshold=\"BLOCK_ONLY_HIGH\",\n",
    "                )\n",
    "            ],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Depending on SDK version, parts may hang off resp.candidates[0].content.parts\n",
    "    parts = getattr(resp, \"parts\", None)\n",
    "    if not parts and getattr(resp, \"candidates\", None):\n",
    "        try:\n",
    "            parts = resp.candidates[0].content.parts\n",
    "        except Exception:\n",
    "            parts = None\n",
    "\n",
    "    if not parts:\n",
    "        raise RuntimeError(\"No image content returned (safety/quota/SDK change).\")\n",
    "\n",
    "    for part in parts:\n",
    "        inline = getattr(part, \"inline_data\", None)\n",
    "        if inline and getattr(inline, \"data\", None):\n",
    "            return inline.data\n",
    "\n",
    "    raise RuntimeError(\"No inline image bytes found in response.\")\n",
    "\n",
    "\n",
    "def generate_and_place_images(state: State) -> dict:\n",
    "\n",
    "    plan = state[\"plan\"]\n",
    "    assert plan is not None\n",
    "\n",
    "    md = state.get(\"md_with_placeholders\") or state[\"merged_md\"]\n",
    "    image_specs = state.get(\"image_specs\", []) or []\n",
    "\n",
    "    # If no images requested, just write merged markdown\n",
    "    if not image_specs:\n",
    "        filename = f\"{plan.blog_title}.md\"\n",
    "        Path(filename).write_text(md, encoding=\"utf-8\")\n",
    "        return {\"final\": md}\n",
    "\n",
    "    images_dir = Path(\"images\")\n",
    "    images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for spec in image_specs:\n",
    "        placeholder = spec[\"placeholder\"]\n",
    "        filename = spec[\"filename\"]\n",
    "        out_path = images_dir / filename\n",
    "\n",
    "        # generate only if needed\n",
    "        if not out_path.exists():\n",
    "            try:\n",
    "                img_bytes = _gemini_generate_image_bytes(spec[\"prompt\"])\n",
    "                out_path.write_bytes(img_bytes)\n",
    "            except Exception as e:\n",
    "                # graceful fallback: keep doc usable\n",
    "                prompt_block = (\n",
    "                    f\"> **[IMAGE GENERATION FAILED]** {spec.get('caption','')}\\n>\\n\"\n",
    "                    f\"> **Alt:** {spec.get('alt','')}\\n>\\n\"\n",
    "                    f\"> **Prompt:** {spec.get('prompt','')}\\n>\\n\"\n",
    "                    f\"> **Error:** {e}\\n\"\n",
    "                )\n",
    "                md = md.replace(placeholder, prompt_block)\n",
    "                continue\n",
    "\n",
    "        img_md = f\"![{spec['alt']}](images/{filename})\\n*{spec['caption']}*\"\n",
    "        md = md.replace(placeholder, img_md)\n",
    "\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    Path(filename).write_text(md, encoding=\"utf-8\")\n",
    "    return {\"final\": md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cebc44ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x12c6ad7d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build reducer subgraph\n",
    "reducer_graph = StateGraph(State)\n",
    "reducer_graph.add_node(\"merge_content\", merge_content)\n",
    "reducer_graph.add_node(\"decide_images\", decide_images)\n",
    "reducer_graph.add_node(\"generate_and_place_images\", generate_and_place_images)\n",
    "reducer_graph.add_edge(START, \"merge_content\")\n",
    "reducer_graph.add_edge(\"merge_content\", \"decide_images\")\n",
    "reducer_graph.add_edge(\"decide_images\", \"generate_and_place_images\")\n",
    "reducer_graph.add_edge(\"generate_and_place_images\", END)\n",
    "reducer_subgraph = reducer_graph.compile()\n",
    "\n",
    "reducer_subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b41ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x12c6d0b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build main graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_subgraph)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2f5a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str, as_of: Optional[str] = None):\n",
    "    if as_of is None:\n",
    "        as_of = date.today().isoformat()\n",
    "\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"as_of\": as_of,\n",
    "            \"recency_days\": 7,\n",
    "            \"sections\": [],\n",
    "            \"merged_md\": \"\",\n",
    "            \"md_with_placeholders\": \"\",\n",
    "            \"image_specs\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c066987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Self Attention in Transformer Architecture',\n",
       " 'mode': 'closed_book',\n",
       " 'needs_research': False,\n",
       " 'queries': [],\n",
       " 'evidence': [],\n",
       " 'plan': Plan(blog_title='Understanding Self-Attention in Transformer Architecture', audience='Developers and Machine Learning Engineers familiar with neural networks and looking to deepen their understanding of transformers', tone='Technical and instructive', blog_kind='explainer', constraints=['Focus on actionable understanding without relying on citations', 'Include minimal working examples', 'Cover both theoretical and practical considerations', 'Explain debugging and edge cases'], tasks=[Task(id=1, title='Introduction to Self-Attention in Transformers', goal='Explain what self-attention is and its role within transformer architecture.', bullets=['Define self-attention conceptually and differentiate it from traditional attention mechanisms', 'Describe the role of self-attention in capturing dependencies within input data', 'Explain how self-attention contributes to parallel processing in transformers', 'Highlight the advantage of self-attention over RNNs/LSTMs in handling sequence data efficiently'], target_words=350, tags=['concept', 'background'], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='Mathematics Behind the Self-Attention Mechanism', goal='Break down the mathematical operations that constitute self-attention so readers can implement and debug it.', bullets=['Describe input representations as Query, Key, and Value matrices', 'Explain how attention scores are calculated using dot products and scaled by the dimension', 'Discuss the softmax normalization and its importance for attention weights', 'Outline how weighted sum using attention weights produces the output representation', 'Include matrix shapes and basic dimensionality to clarify computation flow'], target_words=450, tags=['mathematics', 'technical_details'], requires_research=False, requires_citations=False, requires_code=True), Task(id=3, title='Minimal Code Example Demonstrating Self-Attention Computation', goal='Provide a concise, runnable code sketch of self-attention to concretize understanding and aid implementation.', bullets=['Implement basic self-attention using a popular deep learning framework like PyTorch or TensorFlow', 'Map inputs to queries, keys, and values and compute attention scores explicitly', 'Show how to apply softmax and compute the final output', 'Discuss how to test and verify the output correctness with simple input vectors'], target_words=320, tags=['code', 'implementation', 'verification'], requires_research=False, requires_citations=False, requires_code=True), Task(id=4, title='Design Considerations and Performance Implications of Self-Attention', goal='Analyze how self-attention affects transformer design, performance, and resource costs.', bullets=['Discuss the computational complexity of self-attention in terms of input length', 'Compare impact on speed and memory footprint relative to recurrent architectures', 'Highlight batching and parallelization advantages due to attention matrix operations', 'Consider practical constraints in scaling self-attention e.g. long sequence limits', 'Mention trade-offs between accuracy and efficiency in attention mechanisms'], target_words=400, tags=['performance', 'scalability', 'cost'], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Common Edge Cases and Debugging Tips for Self-Attention Implementations', goal='Guide readers to identify and resolve typical bugs and pitfalls when implementing self-attention.', bullets=['List frequent errors such as shape mismatches, incorrect scaling factor, or mishandling masked attention', 'Explain how to use intermediate tensor inspections and visualization to debug attention scores', 'Recommend steps to verify numerical stability, e.g. softmax overflow or underflow', 'Discuss handling variable-length sequences and padding correctly within attention computations', 'Offer tips for unit testing with controlled inputs and expected outputs'], target_words=350, tags=['debugging', 'edge_cases', 'best_practices'], requires_research=False, requires_citations=False, requires_code=True)]),\n",
       " 'as_of': '2026-01-31',\n",
       " 'recency_days': 7,\n",
       " 'sections': [(1,\n",
       "   \"## Introduction to Self-Attention in Transformers\\n\\nSelf-attention is a specialized attention mechanism where a sequence's elements attend to each other within the same input sequence, rather than relying on an external context as in traditional attention. Unlike classic attention, which typically relates one source sequence to a different target sequence (e.g., in encoder-decoder models), self-attention operates intra-sequence, allowing each token to dynamically weight and integrate information from all other tokens in that sequence.\\n\\nIn the transformer architecture, self-attention plays a crucial role in capturing dependencies between tokens regardless of their distance. This capability is essential for understanding context where relationships are non-local, such as in long sentences or documents. By calculating attention scores between every pair of tokens, the model learns to highlight relevant information across the entire input, enabling richer and more nuanced representations compared to simple fixed-window or sequential models.\\n\\nOne of the key practical benefits of self-attention is its suitability for parallel computation. Since dependencies among all tokens are processed simultaneously through matrix operations, transformers avoid the bottleneck of sequential token processing that characterizes RNNs and LSTMs. This parallelism leads to significant efficiency improvements, especially for long sequences, and makes training on modern hardware accelerators like GPUs and TPUs more effective.\\n\\nMoreover, self-attention models outperform RNNs/LSTMs in handling long-range dependencies because they do not rely on stepwise propagation of information across tokens. RNN-based architectures often struggle with vanishing gradients and fixed-length memory, which limit their ability to remember distant context. Self-attention bypasses these issues by directly modelling token-to-token interactions, enabling both better gradient flow and more flexible context utilization.\\n\\nIn summary, self-attention is the core mechanism enabling transformers to efficiently and effectively understand complex sequence data. It unifies context modeling across all positions, facilitates parallel execution, and overcomes legacy issues in sequential models, making it indispensable for modern deep learning sequence tasks.\"),\n",
       "  (2,\n",
       "   '## Mathematics Behind the Self-Attention Mechanism\\n\\nAt the core of the transformer architecture is the self-attention mechanism, which dynamically computes contextual relationships within an input sequence. Understanding its mathematics is crucial for implementing and debugging transformers effectively.\\n\\n### Input Representations: Query, Key, and Value Matrices\\n\\nThe self-attention mechanism operates on three distinct but related representations derived from the input embeddings (typically word or token embeddings):\\n\\n- **Query (Q):** Represents the set of vectors the model uses to query the input sequence.\\n- **Key (K):** Represents the sequence elements that each query will attend to.\\n- **Value (V):** Contains the actual data to be aggregated based on attention scores.\\n\\nUsually, these are obtained by multiplying the input embedding matrix \\\\( X \\\\in \\\\mathbb{R}^{n \\\\times d_{model}} \\\\), where \\\\( n \\\\) is the sequence length and \\\\( d_{model} \\\\) the embedding dimension, by learned projection matrices \\\\( W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d_{model} \\\\times d_k} \\\\):\\n\\n\\\\[\\nQ = X W_Q, \\\\quad K = X W_K, \\\\quad V = X W_V\\n\\\\]\\n\\nHere, \\\\( d_k \\\\) is the dimensionality of the queries and keys, often set such that \\\\( d_k = d_v = d_{model} / h \\\\) when using multiple attention heads (\\\\( h \\\\)).\\n\\n### Calculating Attention Scores\\n\\nAttention scores quantify the compatibility between each Query and Key pair via the dot product:\\n\\n\\\\[\\n\\\\text{scores} = Q K^\\\\top\\n\\\\]\\n\\nSince \\\\( Q \\\\in \\\\mathbb{R}^{n \\\\times d_k} \\\\) and \\\\( K^\\\\top \\\\in \\\\mathbb{R}^{d_k \\\\times n} \\\\), the resulting \\\\( \\\\text{scores} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) matrix expresses how much each token should attend to every other token.\\n\\nTo stabilize gradients and prevent excessively large dot products, scores are scaled by \\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_scores} = \\\\frac{Q K^\\\\top}{\\\\sqrt{d_k}}\\n\\\\]\\n\\nThis scaling is essential because the magnitude of dot products grows with \\\\( d_k \\\\), which can lead to softmax saturation and vanishing gradients.\\n\\n### Softmax Normalization and Attention Weights\\n\\nThe scaled scores are normalized using the softmax function along each Query\\'s dimension to produce attention weights:\\n\\n\\\\[\\n\\\\text{attention\\\\_weights}_{i,j} = \\\\frac{\\\\exp(\\\\text{scaled\\\\_scores}_{i,j})}{\\\\sum_{k=1}^{n} \\\\exp(\\\\text{scaled\\\\_scores}_{i,k})}\\n\\\\]\\n\\nThis ensures that for each query \\\\( i \\\\), the weights sum to 1, forming a valid probability distribution over keys \\\\( j \\\\). Normalization is critical to emphasize the most relevant keys while reducing noise from less relevant ones.\\n\\n### Weighted Sum Producing the Output Representation\\n\\nFinally, the output for each position is a weighted sum of the Values \\\\( V \\\\), using the computed attention weights:\\n\\n\\\\[\\n\\\\text{output} = \\\\text{attention\\\\_weights} \\\\times V\\n\\\\]\\n\\nGiven \\\\( \\\\text{attention\\\\_weights} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) and \\\\( V \\\\in \\\\mathbb{R}^{n \\\\times d_v} \\\\), the output has shape \\\\( n \\\\times d_v \\\\), providing contextually enriched representations for each token by aggregating information based on learned attention patterns.\\n\\n### Summary of Shapes and Flow\\n\\n| Variable           | Shape                      | Description                          |\\n|--------------------|----------------------------|------------------------------------|\\n| \\\\( X \\\\)            | \\\\( n \\\\times d_{model} \\\\)    | Input embeddings                   |\\n| \\\\( W_Q, W_K, W_V \\\\) | \\\\( d_{model} \\\\times d_k \\\\)  | Learned projection matrices        |\\n| \\\\( Q, K, V \\\\)      | \\\\( n \\\\times d_k \\\\)          | Projected Queries, Keys, Values    |\\n| Scores             | \\\\( n \\\\times n \\\\)            | Dot product of Q and \\\\( K^\\\\top \\\\)  |\\n| Attention Weights  | \\\\( n \\\\times n \\\\)            | Softmax-normalized scores          |\\n| Output             | \\\\( n \\\\times d_v \\\\)          | Weighted sum of values             |\\n\\n### Minimal Working Example in PyTorch\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef self_attention(X, W_Q, W_K, W_V):\\n    \"\"\"\\n    Computes self-attention output for input tensor X.\\n\\n    Args:\\n        X: Input embeddings, shape (n, d_model)\\n        W_Q, W_K, W_V: Projection matrices, shape (d_model, d_k)\\n\\n    Returns:\\n        output: Self-attention output, shape (n, d_k)\\n    \"\"\"\\n    Q = X @ W_Q            # (n, d_k)\\n    K = X @ W_K            # (n, d_k)\\n    V = X @ W_V            # (n, d_k)\\n\\n    d_k = Q.shape[-1]\\n    scores = Q @ K.T       # (n, n)\\n    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n\\n    attention_weights = F.softmax(scaled_scores, dim=1)  # Normalize rows\\n    output = attention_weights @ V                        # (n, d_k)\\n\\n    return output\\n\\n# Example usage:\\nn, d_model, d_k = 5, 512, 64\\nX = torch.rand(n, d_model)\\nW_Q = torch.rand(d_model, d_k)\\nW_K = torch.rand(d_model, d_k)\\nW_V = torch.rand(d_model, d_k)\\n\\noutput = self_attention(X, W_Q, W_K, W_V)\\nprint(output.shape)  # Expected: (5, 64)\\n```\\n\\nThis example highlights how matrix multiplications and softmax normalization are combined to implement self-attention. Understanding these steps allows you to debug transformers effectively, especially checking shapes at each step and ensuring proper scaling before softmax.'),\n",
       "  (3,\n",
       "   '## Minimal Code Example Demonstrating Self-Attention Computation\\n\\nTo concretize the concept of self-attention and facilitate implementation, here is a minimal runnable example of self-attention using PyTorch. This example explicitly maps inputs to queries (Q), keys (K), and values (V), computes scaled dot-product attention scores, applies the softmax normalization, and produces the final output.\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\n# Define input: batch_size=1, seq_len=3, embed_dim=4\\nx = torch.tensor([[[1., 0., 1., 0.],\\n                   [0., 2., 0., 2.],\\n                   [1., 1., 1., 1.]]])  # shape: (1, 3, 4)\\n\\n# Initialize simple linear layers to create Q, K, V from input\\n# Here weights are identity matrices for clarity\\nW_q = torch.eye(4)\\nW_k = torch.eye(4)\\nW_v = torch.eye(4)\\n\\n# Compute Q, K, V\\nQ = torch.matmul(x, W_q)  # shape: (1, 3, 4)\\nK = torch.matmul(x, W_k)  # shape: (1, 3, 4)\\nV = torch.matmul(x, W_v)  # shape: (1, 3, 4)\\n\\n# Step 1: Compute attention scores with scaled dot product\\nd_k = Q.size(-1)  # embedding dimension\\nscores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n# scores shape: (1, 3, 3)\\n\\n# Step 2: Apply softmax to obtain attention weights\\nattn_weights = F.softmax(scores, dim=-1)  # shape: (1, 3, 3)\\n\\n# Step 3: Compute output by weighted sum of values\\noutput = torch.matmul(attn_weights, V)  # shape: (1, 3, 4)\\n\\nprint(\"Attention scores:\\\\n\", scores)\\nprint(\"Attention weights (after softmax):\\\\n\", attn_weights)\\nprint(\"Self-attention output:\\\\n\", output)\\n```\\n\\n### Explanation and Verification\\n\\n- **Mapping Inputs to Q, K, V**: Here, for didactic purposes, we use identity matrices as weights, making Q, K, and V identical to the input. This simplifies understanding of the attention computation.\\n- **Scaled Dot-Product Calculation**: Scores are computed by multiplying Q with the transpose of K and scaling by the square root of the embedding dimension, which stabilizes gradients during training.\\n- **Softmax Application**: The softmax converts raw scores into probabilities that sum to 1 across the sequence length dimension, reflecting how much attention each token pays to others.\\n- **Output Computation**: The weighted sum of values V, weighted by attention probabilities, produces the self-attention output.\\n\\n### Testing and Debugging Tips\\n\\n- Start with small, easily interpretable inputs like above to manually verify intermediate values.\\n- Check shapes at every step to ensure matrix multiplications are consistent.\\n- Verify that each row of `attn_weights` sums to 1 (due to softmax).\\n- Test edge cases such as identical input vectors or zero vectors to observe expected outputs, e.g., uniform attention distributions or zeros.\\n- Compare outputs with manually computed expected values or use PyTorch’s built-in `nn.MultiheadAttention` as a reference to debug your implementation.\\n\\nThis minimal example lays the groundwork for extending to multi-head attention and integrating into full transformer layers.'),\n",
       "  (4,\n",
       "   '## Design Considerations and Performance Implications of Self-Attention\\n\\nSelf-attention lies at the core of transformer architectures, significantly shaping their design and performance characteristics. Understanding its computational behavior and resource implications is crucial for effective model development and deployment.\\n\\n### Computational Complexity\\n\\nThe self-attention mechanism computes attention scores between every pair of tokens in the input sequence. This results in a quadratic complexity with respect to the input length \\\\( N \\\\), specifically \\\\( O(N^2) \\\\). For each token, attention weights are calculated relative to all other tokens, creating an \\\\( N \\\\times N \\\\) attention matrix. While this enables rich contextual relations, it also leads to rapid growth in compute and memory requirements as input sequences lengthen.\\n\\n### Comparison with Recurrent Architectures\\n\\nUnlike recurrent neural networks (RNNs) that process tokens sequentially, self-attention processes the entire sequence simultaneously. This difference has two major impacts:\\n\\n- **Speed**: Self-attention enables parallel computation across all tokens at once, often resulting in faster training and inference times compared to RNNs that are inherently sequential.\\n- **Memory footprint**: The trade-off comes in memory use, where storing the attention matrix and intermediate activations can be substantially more demanding than the usually linear memory profile of RNNs.\\n\\n### Batching and Parallelization Advantages\\n\\nSelf-attention’s matrix operations lend themselves well to hardware acceleration. GPUs and TPUs can exploit massive parallelism when computing the dot products and softmax operations across the attention matrix. This capability allows batching of multiple sequences to maximize throughput, improving overall scalability. Parallelization both within and across sequences provides a strong practical advantage over recurrent alternatives, which often struggle to parallelize sequence steps.\\n\\n### Practical Constraints in Scaling\\n\\nDespite parallelizable computation, quadratic growth poses challenges for very long sequences. Common practical constraints include:\\n\\n- **Memory limits**: Large attention matrices become infeasible for inputs with thousands of tokens or more.\\n- **Latency considerations**: Slower performance due to larger matrix operations can affect real-time applications.\\n\\nTo cope, transformer implementations typically:\\n\\n- Limit maximum sequence lengths.\\n- Use sparse or approximate attention variants to reduce complexity.\\n- Apply techniques like windowing or chunking sequences.\\n\\n### Trade-offs Between Accuracy and Efficiency\\n\\nThe choice of attention mechanism involves balancing accuracy and computational cost. Full self-attention maximizes contextual relationships but at higher resource expense. Alternatives include:\\n\\n- **Sparse attention**: Restricting attention to local windows or fixed patterns reduces complexity to near linear but may miss global context.\\n- **Low-rank or kernel-based approximations**: These methods further reduce cost but can degrade model quality if not carefully tuned.\\n\\nModel designers must consider the application’s accuracy requirements alongside available hardware and latency constraints to select the appropriate attention variant, optimizing for both performance and resource efficiency.'),\n",
       "  (5,\n",
       "   '## Common Edge Cases and Debugging Tips for Self-Attention Implementations\\n\\nWhen implementing self-attention, developers often encounter subtle bugs that can significantly degrade performance or cause runtime errors. Here are frequent errors and actionable debugging tips to help you build robust self-attention modules.\\n\\n### Frequent Errors to Watch For\\n- **Shape mismatches:** Queries, keys, and values must have compatible shapes. Remember the typical shape convention: `(batch_size, seq_length, embed_dim)`. Mixing up dimensions or batch vs. sequence axes is a common source of errors.\\n- **Incorrect scaling factor:** The attention logits should be scaled by `1 / sqrt(d_k)` where `d_k` is the dimensionality of the key vectors. Forgetting or miscalculating this results in poor gradient behavior.\\n- **Mishandling masked attention:** When applying masks for padding or causal attention, ensure the mask is broadcast correctly and applied before softmax to avoid leaking information or introducing NaNs.\\n\\n### Debugging with Intermediate Tensor Inspection\\nInsert debug statements to print or log intermediate tensors such as raw attention scores, post-mask logits, and attention weights. Visualizing these tensors can reveal unexpected values or distributions:\\n\\n```python\\nprint(\"Attention logits shape:\", logits.shape)\\nprint(\"Logits sample:\", logits[0, :5, :5])\\n```\\n\\nVisual tools like heatmaps can also help understand how attention weights are spread across tokens.\\n\\n### Verifying Numerical Stability\\nSoftmax can suffer overflow with large logits, leading to NaNs or Inf values:\\n\\n- Always subtract the max logit value from each logit vector before applying softmax:\\n  \\n  ```python\\n  logits = logits - logits.max(dim=-1, keepdim=True)[0]\\n  attention_weights = torch.softmax(logits, dim=-1)\\n  ```\\n\\n- Check for very large or very small logits during training to catch unstable inputs early.\\n\\n### Handling Variable-Length Sequences and Padding\\nTo correctly process batches with variable-length sequences:\\n\\n- Pad sequences to the same length.\\n- Create a mask that indicates valid tokens (`1`) vs. padding (`0`).\\n- Apply this mask by setting logits of padded tokens to a large negative number (e.g. `-1e9`) before softmax, ensuring they get negligible attention weights.\\n\\n### Unit Testing with Controlled Inputs\\nValidate your self-attention code using small, deterministic inputs where you can compute expected outputs by hand or from a reference implementation:\\n\\n- Test with identical queries and keys to verify attention peaks at correct positions.\\n- Use simple binary masks to check masking behavior.\\n- Compare shapes and verify softmax outputs sum to 1 along the correct axis.\\n\\n```python\\ndef test_attention_sum_to_one():\\n    queries = torch.ones(1, 3, 4)\\n    keys = torch.ones(1, 3, 4)\\n    values = torch.ones(1, 3, 4)\\n    mask = torch.tensor([[1,1,0]], dtype=torch.bool)\\n    attn_output, attn_weights = self_attention(queries, keys, values, mask)\\n    assert torch.allclose(attn_weights.sum(dim=-1), torch.tensor([1., 1., 0.]), atol=1e-5)\\n```\\n\\nThese targeted checks help catch common pitfalls early and ensure your self-attention layer behaves as expected in practice.'),\n",
       "  (1,\n",
       "   \"## Introduction to Self-Attention in Transformers\\n\\nSelf-attention is a specialized attention mechanism where a sequence's elements attend to each other within the same input sequence, rather than relying on an external context as in traditional attention. Unlike classic attention, which typically relates one source sequence to a different target sequence (e.g., in encoder-decoder models), self-attention operates intra-sequence, allowing each token to dynamically weight and integrate information from all other tokens in that sequence.\\n\\nIn the transformer architecture, self-attention plays a crucial role in capturing dependencies between tokens regardless of their distance. This capability is essential for understanding context where relationships are non-local, such as in long sentences or documents. By calculating attention scores between every pair of tokens, the model learns to highlight relevant information across the entire input, enabling richer and more nuanced representations compared to simple fixed-window or sequential models.\\n\\nOne of the key practical benefits of self-attention is its suitability for parallel computation. Since dependencies among all tokens are processed simultaneously through matrix operations, transformers avoid the bottleneck of sequential token processing that characterizes RNNs and LSTMs. This parallelism leads to significant efficiency improvements, especially for long sequences, and makes training on modern hardware accelerators like GPUs and TPUs more effective.\\n\\nMoreover, self-attention models outperform RNNs/LSTMs in handling long-range dependencies because they do not rely on stepwise propagation of information across tokens. RNN-based architectures often struggle with vanishing gradients and fixed-length memory, which limit their ability to remember distant context. Self-attention bypasses these issues by directly modelling token-to-token interactions, enabling both better gradient flow and more flexible context utilization.\\n\\nIn summary, self-attention is the core mechanism enabling transformers to efficiently and effectively understand complex sequence data. It unifies context modeling across all positions, facilitates parallel execution, and overcomes legacy issues in sequential models, making it indispensable for modern deep learning sequence tasks.\"),\n",
       "  (2,\n",
       "   '## Mathematics Behind the Self-Attention Mechanism\\n\\nAt the core of the transformer architecture is the self-attention mechanism, which dynamically computes contextual relationships within an input sequence. Understanding its mathematics is crucial for implementing and debugging transformers effectively.\\n\\n### Input Representations: Query, Key, and Value Matrices\\n\\nThe self-attention mechanism operates on three distinct but related representations derived from the input embeddings (typically word or token embeddings):\\n\\n- **Query (Q):** Represents the set of vectors the model uses to query the input sequence.\\n- **Key (K):** Represents the sequence elements that each query will attend to.\\n- **Value (V):** Contains the actual data to be aggregated based on attention scores.\\n\\nUsually, these are obtained by multiplying the input embedding matrix \\\\( X \\\\in \\\\mathbb{R}^{n \\\\times d_{model}} \\\\), where \\\\( n \\\\) is the sequence length and \\\\( d_{model} \\\\) the embedding dimension, by learned projection matrices \\\\( W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d_{model} \\\\times d_k} \\\\):\\n\\n\\\\[\\nQ = X W_Q, \\\\quad K = X W_K, \\\\quad V = X W_V\\n\\\\]\\n\\nHere, \\\\( d_k \\\\) is the dimensionality of the queries and keys, often set such that \\\\( d_k = d_v = d_{model} / h \\\\) when using multiple attention heads (\\\\( h \\\\)).\\n\\n### Calculating Attention Scores\\n\\nAttention scores quantify the compatibility between each Query and Key pair via the dot product:\\n\\n\\\\[\\n\\\\text{scores} = Q K^\\\\top\\n\\\\]\\n\\nSince \\\\( Q \\\\in \\\\mathbb{R}^{n \\\\times d_k} \\\\) and \\\\( K^\\\\top \\\\in \\\\mathbb{R}^{d_k \\\\times n} \\\\), the resulting \\\\( \\\\text{scores} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) matrix expresses how much each token should attend to every other token.\\n\\nTo stabilize gradients and prevent excessively large dot products, scores are scaled by \\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_scores} = \\\\frac{Q K^\\\\top}{\\\\sqrt{d_k}}\\n\\\\]\\n\\nThis scaling is essential because the magnitude of dot products grows with \\\\( d_k \\\\), which can lead to softmax saturation and vanishing gradients.\\n\\n### Softmax Normalization and Attention Weights\\n\\nThe scaled scores are normalized using the softmax function along each Query\\'s dimension to produce attention weights:\\n\\n\\\\[\\n\\\\text{attention\\\\_weights}_{i,j} = \\\\frac{\\\\exp(\\\\text{scaled\\\\_scores}_{i,j})}{\\\\sum_{k=1}^{n} \\\\exp(\\\\text{scaled\\\\_scores}_{i,k})}\\n\\\\]\\n\\nThis ensures that for each query \\\\( i \\\\), the weights sum to 1, forming a valid probability distribution over keys \\\\( j \\\\). Normalization is critical to emphasize the most relevant keys while reducing noise from less relevant ones.\\n\\n### Weighted Sum Producing the Output Representation\\n\\nFinally, the output for each position is a weighted sum of the Values \\\\( V \\\\), using the computed attention weights:\\n\\n\\\\[\\n\\\\text{output} = \\\\text{attention\\\\_weights} \\\\times V\\n\\\\]\\n\\nGiven \\\\( \\\\text{attention\\\\_weights} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) and \\\\( V \\\\in \\\\mathbb{R}^{n \\\\times d_v} \\\\), the output has shape \\\\( n \\\\times d_v \\\\), providing contextually enriched representations for each token by aggregating information based on learned attention patterns.\\n\\n### Summary of Shapes and Flow\\n\\n| Variable           | Shape                      | Description                          |\\n|--------------------|----------------------------|------------------------------------|\\n| \\\\( X \\\\)            | \\\\( n \\\\times d_{model} \\\\)    | Input embeddings                   |\\n| \\\\( W_Q, W_K, W_V \\\\) | \\\\( d_{model} \\\\times d_k \\\\)  | Learned projection matrices        |\\n| \\\\( Q, K, V \\\\)      | \\\\( n \\\\times d_k \\\\)          | Projected Queries, Keys, Values    |\\n| Scores             | \\\\( n \\\\times n \\\\)            | Dot product of Q and \\\\( K^\\\\top \\\\)  |\\n| Attention Weights  | \\\\( n \\\\times n \\\\)            | Softmax-normalized scores          |\\n| Output             | \\\\( n \\\\times d_v \\\\)          | Weighted sum of values             |\\n\\n### Minimal Working Example in PyTorch\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef self_attention(X, W_Q, W_K, W_V):\\n    \"\"\"\\n    Computes self-attention output for input tensor X.\\n\\n    Args:\\n        X: Input embeddings, shape (n, d_model)\\n        W_Q, W_K, W_V: Projection matrices, shape (d_model, d_k)\\n\\n    Returns:\\n        output: Self-attention output, shape (n, d_k)\\n    \"\"\"\\n    Q = X @ W_Q            # (n, d_k)\\n    K = X @ W_K            # (n, d_k)\\n    V = X @ W_V            # (n, d_k)\\n\\n    d_k = Q.shape[-1]\\n    scores = Q @ K.T       # (n, n)\\n    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n\\n    attention_weights = F.softmax(scaled_scores, dim=1)  # Normalize rows\\n    output = attention_weights @ V                        # (n, d_k)\\n\\n    return output\\n\\n# Example usage:\\nn, d_model, d_k = 5, 512, 64\\nX = torch.rand(n, d_model)\\nW_Q = torch.rand(d_model, d_k)\\nW_K = torch.rand(d_model, d_k)\\nW_V = torch.rand(d_model, d_k)\\n\\noutput = self_attention(X, W_Q, W_K, W_V)\\nprint(output.shape)  # Expected: (5, 64)\\n```\\n\\nThis example highlights how matrix multiplications and softmax normalization are combined to implement self-attention. Understanding these steps allows you to debug transformers effectively, especially checking shapes at each step and ensuring proper scaling before softmax.'),\n",
       "  (3,\n",
       "   '## Minimal Code Example Demonstrating Self-Attention Computation\\n\\nTo concretize the concept of self-attention and facilitate implementation, here is a minimal runnable example of self-attention using PyTorch. This example explicitly maps inputs to queries (Q), keys (K), and values (V), computes scaled dot-product attention scores, applies the softmax normalization, and produces the final output.\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\n# Define input: batch_size=1, seq_len=3, embed_dim=4\\nx = torch.tensor([[[1., 0., 1., 0.],\\n                   [0., 2., 0., 2.],\\n                   [1., 1., 1., 1.]]])  # shape: (1, 3, 4)\\n\\n# Initialize simple linear layers to create Q, K, V from input\\n# Here weights are identity matrices for clarity\\nW_q = torch.eye(4)\\nW_k = torch.eye(4)\\nW_v = torch.eye(4)\\n\\n# Compute Q, K, V\\nQ = torch.matmul(x, W_q)  # shape: (1, 3, 4)\\nK = torch.matmul(x, W_k)  # shape: (1, 3, 4)\\nV = torch.matmul(x, W_v)  # shape: (1, 3, 4)\\n\\n# Step 1: Compute attention scores with scaled dot product\\nd_k = Q.size(-1)  # embedding dimension\\nscores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n# scores shape: (1, 3, 3)\\n\\n# Step 2: Apply softmax to obtain attention weights\\nattn_weights = F.softmax(scores, dim=-1)  # shape: (1, 3, 3)\\n\\n# Step 3: Compute output by weighted sum of values\\noutput = torch.matmul(attn_weights, V)  # shape: (1, 3, 4)\\n\\nprint(\"Attention scores:\\\\n\", scores)\\nprint(\"Attention weights (after softmax):\\\\n\", attn_weights)\\nprint(\"Self-attention output:\\\\n\", output)\\n```\\n\\n### Explanation and Verification\\n\\n- **Mapping Inputs to Q, K, V**: Here, for didactic purposes, we use identity matrices as weights, making Q, K, and V identical to the input. This simplifies understanding of the attention computation.\\n- **Scaled Dot-Product Calculation**: Scores are computed by multiplying Q with the transpose of K and scaling by the square root of the embedding dimension, which stabilizes gradients during training.\\n- **Softmax Application**: The softmax converts raw scores into probabilities that sum to 1 across the sequence length dimension, reflecting how much attention each token pays to others.\\n- **Output Computation**: The weighted sum of values V, weighted by attention probabilities, produces the self-attention output.\\n\\n### Testing and Debugging Tips\\n\\n- Start with small, easily interpretable inputs like above to manually verify intermediate values.\\n- Check shapes at every step to ensure matrix multiplications are consistent.\\n- Verify that each row of `attn_weights` sums to 1 (due to softmax).\\n- Test edge cases such as identical input vectors or zero vectors to observe expected outputs, e.g., uniform attention distributions or zeros.\\n- Compare outputs with manually computed expected values or use PyTorch’s built-in `nn.MultiheadAttention` as a reference to debug your implementation.\\n\\nThis minimal example lays the groundwork for extending to multi-head attention and integrating into full transformer layers.'),\n",
       "  (4,\n",
       "   '## Design Considerations and Performance Implications of Self-Attention\\n\\nSelf-attention lies at the core of transformer architectures, significantly shaping their design and performance characteristics. Understanding its computational behavior and resource implications is crucial for effective model development and deployment.\\n\\n### Computational Complexity\\n\\nThe self-attention mechanism computes attention scores between every pair of tokens in the input sequence. This results in a quadratic complexity with respect to the input length \\\\( N \\\\), specifically \\\\( O(N^2) \\\\). For each token, attention weights are calculated relative to all other tokens, creating an \\\\( N \\\\times N \\\\) attention matrix. While this enables rich contextual relations, it also leads to rapid growth in compute and memory requirements as input sequences lengthen.\\n\\n### Comparison with Recurrent Architectures\\n\\nUnlike recurrent neural networks (RNNs) that process tokens sequentially, self-attention processes the entire sequence simultaneously. This difference has two major impacts:\\n\\n- **Speed**: Self-attention enables parallel computation across all tokens at once, often resulting in faster training and inference times compared to RNNs that are inherently sequential.\\n- **Memory footprint**: The trade-off comes in memory use, where storing the attention matrix and intermediate activations can be substantially more demanding than the usually linear memory profile of RNNs.\\n\\n### Batching and Parallelization Advantages\\n\\nSelf-attention’s matrix operations lend themselves well to hardware acceleration. GPUs and TPUs can exploit massive parallelism when computing the dot products and softmax operations across the attention matrix. This capability allows batching of multiple sequences to maximize throughput, improving overall scalability. Parallelization both within and across sequences provides a strong practical advantage over recurrent alternatives, which often struggle to parallelize sequence steps.\\n\\n### Practical Constraints in Scaling\\n\\nDespite parallelizable computation, quadratic growth poses challenges for very long sequences. Common practical constraints include:\\n\\n- **Memory limits**: Large attention matrices become infeasible for inputs with thousands of tokens or more.\\n- **Latency considerations**: Slower performance due to larger matrix operations can affect real-time applications.\\n\\nTo cope, transformer implementations typically:\\n\\n- Limit maximum sequence lengths.\\n- Use sparse or approximate attention variants to reduce complexity.\\n- Apply techniques like windowing or chunking sequences.\\n\\n### Trade-offs Between Accuracy and Efficiency\\n\\nThe choice of attention mechanism involves balancing accuracy and computational cost. Full self-attention maximizes contextual relationships but at higher resource expense. Alternatives include:\\n\\n- **Sparse attention**: Restricting attention to local windows or fixed patterns reduces complexity to near linear but may miss global context.\\n- **Low-rank or kernel-based approximations**: These methods further reduce cost but can degrade model quality if not carefully tuned.\\n\\nModel designers must consider the application’s accuracy requirements alongside available hardware and latency constraints to select the appropriate attention variant, optimizing for both performance and resource efficiency.'),\n",
       "  (5,\n",
       "   '## Common Edge Cases and Debugging Tips for Self-Attention Implementations\\n\\nWhen implementing self-attention, developers often encounter subtle bugs that can significantly degrade performance or cause runtime errors. Here are frequent errors and actionable debugging tips to help you build robust self-attention modules.\\n\\n### Frequent Errors to Watch For\\n- **Shape mismatches:** Queries, keys, and values must have compatible shapes. Remember the typical shape convention: `(batch_size, seq_length, embed_dim)`. Mixing up dimensions or batch vs. sequence axes is a common source of errors.\\n- **Incorrect scaling factor:** The attention logits should be scaled by `1 / sqrt(d_k)` where `d_k` is the dimensionality of the key vectors. Forgetting or miscalculating this results in poor gradient behavior.\\n- **Mishandling masked attention:** When applying masks for padding or causal attention, ensure the mask is broadcast correctly and applied before softmax to avoid leaking information or introducing NaNs.\\n\\n### Debugging with Intermediate Tensor Inspection\\nInsert debug statements to print or log intermediate tensors such as raw attention scores, post-mask logits, and attention weights. Visualizing these tensors can reveal unexpected values or distributions:\\n\\n```python\\nprint(\"Attention logits shape:\", logits.shape)\\nprint(\"Logits sample:\", logits[0, :5, :5])\\n```\\n\\nVisual tools like heatmaps can also help understand how attention weights are spread across tokens.\\n\\n### Verifying Numerical Stability\\nSoftmax can suffer overflow with large logits, leading to NaNs or Inf values:\\n\\n- Always subtract the max logit value from each logit vector before applying softmax:\\n  \\n  ```python\\n  logits = logits - logits.max(dim=-1, keepdim=True)[0]\\n  attention_weights = torch.softmax(logits, dim=-1)\\n  ```\\n\\n- Check for very large or very small logits during training to catch unstable inputs early.\\n\\n### Handling Variable-Length Sequences and Padding\\nTo correctly process batches with variable-length sequences:\\n\\n- Pad sequences to the same length.\\n- Create a mask that indicates valid tokens (`1`) vs. padding (`0`).\\n- Apply this mask by setting logits of padded tokens to a large negative number (e.g. `-1e9`) before softmax, ensuring they get negligible attention weights.\\n\\n### Unit Testing with Controlled Inputs\\nValidate your self-attention code using small, deterministic inputs where you can compute expected outputs by hand or from a reference implementation:\\n\\n- Test with identical queries and keys to verify attention peaks at correct positions.\\n- Use simple binary masks to check masking behavior.\\n- Compare shapes and verify softmax outputs sum to 1 along the correct axis.\\n\\n```python\\ndef test_attention_sum_to_one():\\n    queries = torch.ones(1, 3, 4)\\n    keys = torch.ones(1, 3, 4)\\n    values = torch.ones(1, 3, 4)\\n    mask = torch.tensor([[1,1,0]], dtype=torch.bool)\\n    attn_output, attn_weights = self_attention(queries, keys, values, mask)\\n    assert torch.allclose(attn_weights.sum(dim=-1), torch.tensor([1., 1., 0.]), atol=1e-5)\\n```\\n\\nThese targeted checks help catch common pitfalls early and ensure your self-attention layer behaves as expected in practice.')],\n",
       " 'merged_md': '# Understanding Self-Attention in Transformer Architecture\\n\\n## Introduction to Self-Attention in Transformers\\n\\nSelf-attention is a specialized attention mechanism where a sequence\\'s elements attend to each other within the same input sequence, rather than relying on an external context as in traditional attention. Unlike classic attention, which typically relates one source sequence to a different target sequence (e.g., in encoder-decoder models), self-attention operates intra-sequence, allowing each token to dynamically weight and integrate information from all other tokens in that sequence.\\n\\nIn the transformer architecture, self-attention plays a crucial role in capturing dependencies between tokens regardless of their distance. This capability is essential for understanding context where relationships are non-local, such as in long sentences or documents. By calculating attention scores between every pair of tokens, the model learns to highlight relevant information across the entire input, enabling richer and more nuanced representations compared to simple fixed-window or sequential models.\\n\\nOne of the key practical benefits of self-attention is its suitability for parallel computation. Since dependencies among all tokens are processed simultaneously through matrix operations, transformers avoid the bottleneck of sequential token processing that characterizes RNNs and LSTMs. This parallelism leads to significant efficiency improvements, especially for long sequences, and makes training on modern hardware accelerators like GPUs and TPUs more effective.\\n\\nMoreover, self-attention models outperform RNNs/LSTMs in handling long-range dependencies because they do not rely on stepwise propagation of information across tokens. RNN-based architectures often struggle with vanishing gradients and fixed-length memory, which limit their ability to remember distant context. Self-attention bypasses these issues by directly modelling token-to-token interactions, enabling both better gradient flow and more flexible context utilization.\\n\\nIn summary, self-attention is the core mechanism enabling transformers to efficiently and effectively understand complex sequence data. It unifies context modeling across all positions, facilitates parallel execution, and overcomes legacy issues in sequential models, making it indispensable for modern deep learning sequence tasks.\\n\\n## Mathematics Behind the Self-Attention Mechanism\\n\\nAt the core of the transformer architecture is the self-attention mechanism, which dynamically computes contextual relationships within an input sequence. Understanding its mathematics is crucial for implementing and debugging transformers effectively.\\n\\n### Input Representations: Query, Key, and Value Matrices\\n\\nThe self-attention mechanism operates on three distinct but related representations derived from the input embeddings (typically word or token embeddings):\\n\\n- **Query (Q):** Represents the set of vectors the model uses to query the input sequence.\\n- **Key (K):** Represents the sequence elements that each query will attend to.\\n- **Value (V):** Contains the actual data to be aggregated based on attention scores.\\n\\nUsually, these are obtained by multiplying the input embedding matrix \\\\( X \\\\in \\\\mathbb{R}^{n \\\\times d_{model}} \\\\), where \\\\( n \\\\) is the sequence length and \\\\( d_{model} \\\\) the embedding dimension, by learned projection matrices \\\\( W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d_{model} \\\\times d_k} \\\\):\\n\\n\\\\[\\nQ = X W_Q, \\\\quad K = X W_K, \\\\quad V = X W_V\\n\\\\]\\n\\nHere, \\\\( d_k \\\\) is the dimensionality of the queries and keys, often set such that \\\\( d_k = d_v = d_{model} / h \\\\) when using multiple attention heads (\\\\( h \\\\)).\\n\\n### Calculating Attention Scores\\n\\nAttention scores quantify the compatibility between each Query and Key pair via the dot product:\\n\\n\\\\[\\n\\\\text{scores} = Q K^\\\\top\\n\\\\]\\n\\nSince \\\\( Q \\\\in \\\\mathbb{R}^{n \\\\times d_k} \\\\) and \\\\( K^\\\\top \\\\in \\\\mathbb{R}^{d_k \\\\times n} \\\\), the resulting \\\\( \\\\text{scores} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) matrix expresses how much each token should attend to every other token.\\n\\nTo stabilize gradients and prevent excessively large dot products, scores are scaled by \\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_scores} = \\\\frac{Q K^\\\\top}{\\\\sqrt{d_k}}\\n\\\\]\\n\\nThis scaling is essential because the magnitude of dot products grows with \\\\( d_k \\\\), which can lead to softmax saturation and vanishing gradients.\\n\\n### Softmax Normalization and Attention Weights\\n\\nThe scaled scores are normalized using the softmax function along each Query\\'s dimension to produce attention weights:\\n\\n\\\\[\\n\\\\text{attention\\\\_weights}_{i,j} = \\\\frac{\\\\exp(\\\\text{scaled\\\\_scores}_{i,j})}{\\\\sum_{k=1}^{n} \\\\exp(\\\\text{scaled\\\\_scores}_{i,k})}\\n\\\\]\\n\\nThis ensures that for each query \\\\( i \\\\), the weights sum to 1, forming a valid probability distribution over keys \\\\( j \\\\). Normalization is critical to emphasize the most relevant keys while reducing noise from less relevant ones.\\n\\n### Weighted Sum Producing the Output Representation\\n\\nFinally, the output for each position is a weighted sum of the Values \\\\( V \\\\), using the computed attention weights:\\n\\n\\\\[\\n\\\\text{output} = \\\\text{attention\\\\_weights} \\\\times V\\n\\\\]\\n\\nGiven \\\\( \\\\text{attention\\\\_weights} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) and \\\\( V \\\\in \\\\mathbb{R}^{n \\\\times d_v} \\\\), the output has shape \\\\( n \\\\times d_v \\\\), providing contextually enriched representations for each token by aggregating information based on learned attention patterns.\\n\\n### Summary of Shapes and Flow\\n\\n| Variable           | Shape                      | Description                          |\\n|--------------------|----------------------------|------------------------------------|\\n| \\\\( X \\\\)            | \\\\( n \\\\times d_{model} \\\\)    | Input embeddings                   |\\n| \\\\( W_Q, W_K, W_V \\\\) | \\\\( d_{model} \\\\times d_k \\\\)  | Learned projection matrices        |\\n| \\\\( Q, K, V \\\\)      | \\\\( n \\\\times d_k \\\\)          | Projected Queries, Keys, Values    |\\n| Scores             | \\\\( n \\\\times n \\\\)            | Dot product of Q and \\\\( K^\\\\top \\\\)  |\\n| Attention Weights  | \\\\( n \\\\times n \\\\)            | Softmax-normalized scores          |\\n| Output             | \\\\( n \\\\times d_v \\\\)          | Weighted sum of values             |\\n\\n### Minimal Working Example in PyTorch\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef self_attention(X, W_Q, W_K, W_V):\\n    \"\"\"\\n    Computes self-attention output for input tensor X.\\n\\n    Args:\\n        X: Input embeddings, shape (n, d_model)\\n        W_Q, W_K, W_V: Projection matrices, shape (d_model, d_k)\\n\\n    Returns:\\n        output: Self-attention output, shape (n, d_k)\\n    \"\"\"\\n    Q = X @ W_Q            # (n, d_k)\\n    K = X @ W_K            # (n, d_k)\\n    V = X @ W_V            # (n, d_k)\\n\\n    d_k = Q.shape[-1]\\n    scores = Q @ K.T       # (n, n)\\n    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n\\n    attention_weights = F.softmax(scaled_scores, dim=1)  # Normalize rows\\n    output = attention_weights @ V                        # (n, d_k)\\n\\n    return output\\n\\n# Example usage:\\nn, d_model, d_k = 5, 512, 64\\nX = torch.rand(n, d_model)\\nW_Q = torch.rand(d_model, d_k)\\nW_K = torch.rand(d_model, d_k)\\nW_V = torch.rand(d_model, d_k)\\n\\noutput = self_attention(X, W_Q, W_K, W_V)\\nprint(output.shape)  # Expected: (5, 64)\\n```\\n\\nThis example highlights how matrix multiplications and softmax normalization are combined to implement self-attention. Understanding these steps allows you to debug transformers effectively, especially checking shapes at each step and ensuring proper scaling before softmax.\\n\\n## Minimal Code Example Demonstrating Self-Attention Computation\\n\\nTo concretize the concept of self-attention and facilitate implementation, here is a minimal runnable example of self-attention using PyTorch. This example explicitly maps inputs to queries (Q), keys (K), and values (V), computes scaled dot-product attention scores, applies the softmax normalization, and produces the final output.\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\n# Define input: batch_size=1, seq_len=3, embed_dim=4\\nx = torch.tensor([[[1., 0., 1., 0.],\\n                   [0., 2., 0., 2.],\\n                   [1., 1., 1., 1.]]])  # shape: (1, 3, 4)\\n\\n# Initialize simple linear layers to create Q, K, V from input\\n# Here weights are identity matrices for clarity\\nW_q = torch.eye(4)\\nW_k = torch.eye(4)\\nW_v = torch.eye(4)\\n\\n# Compute Q, K, V\\nQ = torch.matmul(x, W_q)  # shape: (1, 3, 4)\\nK = torch.matmul(x, W_k)  # shape: (1, 3, 4)\\nV = torch.matmul(x, W_v)  # shape: (1, 3, 4)\\n\\n# Step 1: Compute attention scores with scaled dot product\\nd_k = Q.size(-1)  # embedding dimension\\nscores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n# scores shape: (1, 3, 3)\\n\\n# Step 2: Apply softmax to obtain attention weights\\nattn_weights = F.softmax(scores, dim=-1)  # shape: (1, 3, 3)\\n\\n# Step 3: Compute output by weighted sum of values\\noutput = torch.matmul(attn_weights, V)  # shape: (1, 3, 4)\\n\\nprint(\"Attention scores:\\\\n\", scores)\\nprint(\"Attention weights (after softmax):\\\\n\", attn_weights)\\nprint(\"Self-attention output:\\\\n\", output)\\n```\\n\\n### Explanation and Verification\\n\\n- **Mapping Inputs to Q, K, V**: Here, for didactic purposes, we use identity matrices as weights, making Q, K, and V identical to the input. This simplifies understanding of the attention computation.\\n- **Scaled Dot-Product Calculation**: Scores are computed by multiplying Q with the transpose of K and scaling by the square root of the embedding dimension, which stabilizes gradients during training.\\n- **Softmax Application**: The softmax converts raw scores into probabilities that sum to 1 across the sequence length dimension, reflecting how much attention each token pays to others.\\n- **Output Computation**: The weighted sum of values V, weighted by attention probabilities, produces the self-attention output.\\n\\n### Testing and Debugging Tips\\n\\n- Start with small, easily interpretable inputs like above to manually verify intermediate values.\\n- Check shapes at every step to ensure matrix multiplications are consistent.\\n- Verify that each row of `attn_weights` sums to 1 (due to softmax).\\n- Test edge cases such as identical input vectors or zero vectors to observe expected outputs, e.g., uniform attention distributions or zeros.\\n- Compare outputs with manually computed expected values or use PyTorch’s built-in `nn.MultiheadAttention` as a reference to debug your implementation.\\n\\nThis minimal example lays the groundwork for extending to multi-head attention and integrating into full transformer layers.\\n\\n## Design Considerations and Performance Implications of Self-Attention\\n\\nSelf-attention lies at the core of transformer architectures, significantly shaping their design and performance characteristics. Understanding its computational behavior and resource implications is crucial for effective model development and deployment.\\n\\n### Computational Complexity\\n\\nThe self-attention mechanism computes attention scores between every pair of tokens in the input sequence. This results in a quadratic complexity with respect to the input length \\\\( N \\\\), specifically \\\\( O(N^2) \\\\). For each token, attention weights are calculated relative to all other tokens, creating an \\\\( N \\\\times N \\\\) attention matrix. While this enables rich contextual relations, it also leads to rapid growth in compute and memory requirements as input sequences lengthen.\\n\\n### Comparison with Recurrent Architectures\\n\\nUnlike recurrent neural networks (RNNs) that process tokens sequentially, self-attention processes the entire sequence simultaneously. This difference has two major impacts:\\n\\n- **Speed**: Self-attention enables parallel computation across all tokens at once, often resulting in faster training and inference times compared to RNNs that are inherently sequential.\\n- **Memory footprint**: The trade-off comes in memory use, where storing the attention matrix and intermediate activations can be substantially more demanding than the usually linear memory profile of RNNs.\\n\\n### Batching and Parallelization Advantages\\n\\nSelf-attention’s matrix operations lend themselves well to hardware acceleration. GPUs and TPUs can exploit massive parallelism when computing the dot products and softmax operations across the attention matrix. This capability allows batching of multiple sequences to maximize throughput, improving overall scalability. Parallelization both within and across sequences provides a strong practical advantage over recurrent alternatives, which often struggle to parallelize sequence steps.\\n\\n### Practical Constraints in Scaling\\n\\nDespite parallelizable computation, quadratic growth poses challenges for very long sequences. Common practical constraints include:\\n\\n- **Memory limits**: Large attention matrices become infeasible for inputs with thousands of tokens or more.\\n- **Latency considerations**: Slower performance due to larger matrix operations can affect real-time applications.\\n\\nTo cope, transformer implementations typically:\\n\\n- Limit maximum sequence lengths.\\n- Use sparse or approximate attention variants to reduce complexity.\\n- Apply techniques like windowing or chunking sequences.\\n\\n### Trade-offs Between Accuracy and Efficiency\\n\\nThe choice of attention mechanism involves balancing accuracy and computational cost. Full self-attention maximizes contextual relationships but at higher resource expense. Alternatives include:\\n\\n- **Sparse attention**: Restricting attention to local windows or fixed patterns reduces complexity to near linear but may miss global context.\\n- **Low-rank or kernel-based approximations**: These methods further reduce cost but can degrade model quality if not carefully tuned.\\n\\nModel designers must consider the application’s accuracy requirements alongside available hardware and latency constraints to select the appropriate attention variant, optimizing for both performance and resource efficiency.\\n\\n## Common Edge Cases and Debugging Tips for Self-Attention Implementations\\n\\nWhen implementing self-attention, developers often encounter subtle bugs that can significantly degrade performance or cause runtime errors. Here are frequent errors and actionable debugging tips to help you build robust self-attention modules.\\n\\n### Frequent Errors to Watch For\\n- **Shape mismatches:** Queries, keys, and values must have compatible shapes. Remember the typical shape convention: `(batch_size, seq_length, embed_dim)`. Mixing up dimensions or batch vs. sequence axes is a common source of errors.\\n- **Incorrect scaling factor:** The attention logits should be scaled by `1 / sqrt(d_k)` where `d_k` is the dimensionality of the key vectors. Forgetting or miscalculating this results in poor gradient behavior.\\n- **Mishandling masked attention:** When applying masks for padding or causal attention, ensure the mask is broadcast correctly and applied before softmax to avoid leaking information or introducing NaNs.\\n\\n### Debugging with Intermediate Tensor Inspection\\nInsert debug statements to print or log intermediate tensors such as raw attention scores, post-mask logits, and attention weights. Visualizing these tensors can reveal unexpected values or distributions:\\n\\n```python\\nprint(\"Attention logits shape:\", logits.shape)\\nprint(\"Logits sample:\", logits[0, :5, :5])\\n```\\n\\nVisual tools like heatmaps can also help understand how attention weights are spread across tokens.\\n\\n### Verifying Numerical Stability\\nSoftmax can suffer overflow with large logits, leading to NaNs or Inf values:\\n\\n- Always subtract the max logit value from each logit vector before applying softmax:\\n  \\n  ```python\\n  logits = logits - logits.max(dim=-1, keepdim=True)[0]\\n  attention_weights = torch.softmax(logits, dim=-1)\\n  ```\\n\\n- Check for very large or very small logits during training to catch unstable inputs early.\\n\\n### Handling Variable-Length Sequences and Padding\\nTo correctly process batches with variable-length sequences:\\n\\n- Pad sequences to the same length.\\n- Create a mask that indicates valid tokens (`1`) vs. padding (`0`).\\n- Apply this mask by setting logits of padded tokens to a large negative number (e.g. `-1e9`) before softmax, ensuring they get negligible attention weights.\\n\\n### Unit Testing with Controlled Inputs\\nValidate your self-attention code using small, deterministic inputs where you can compute expected outputs by hand or from a reference implementation:\\n\\n- Test with identical queries and keys to verify attention peaks at correct positions.\\n- Use simple binary masks to check masking behavior.\\n- Compare shapes and verify softmax outputs sum to 1 along the correct axis.\\n\\n```python\\ndef test_attention_sum_to_one():\\n    queries = torch.ones(1, 3, 4)\\n    keys = torch.ones(1, 3, 4)\\n    values = torch.ones(1, 3, 4)\\n    mask = torch.tensor([[1,1,0]], dtype=torch.bool)\\n    attn_output, attn_weights = self_attention(queries, keys, values, mask)\\n    assert torch.allclose(attn_weights.sum(dim=-1), torch.tensor([1., 1., 0.]), atol=1e-5)\\n```\\n\\nThese targeted checks help catch common pitfalls early and ensure your self-attention layer behaves as expected in practice.\\n',\n",
       " 'md_with_placeholders': '# Understanding Self-Attention in Transformer Architecture\\n\\n## Introduction to Self-Attention in Transformers\\n\\nSelf-attention is a specialized attention mechanism where a sequence\\'s elements attend to each other within the same input sequence, rather than relying on an external context as in traditional attention. Unlike classic attention, which typically relates one source sequence to a different target sequence (e.g., in encoder-decoder models), self-attention operates intra-sequence, allowing each token to dynamically weight and integrate information from all other tokens in that sequence.\\n\\nIn the transformer architecture, self-attention plays a crucial role in capturing dependencies between tokens regardless of their distance. This capability is essential for understanding context where relationships are non-local, such as in long sentences or documents. By calculating attention scores between every pair of tokens, the model learns to highlight relevant information across the entire input, enabling richer and more nuanced representations compared to simple fixed-window or sequential models.\\n\\nOne of the key practical benefits of self-attention is its suitability for parallel computation. Since dependencies among all tokens are processed simultaneously through matrix operations, transformers avoid the bottleneck of sequential token processing that characterizes RNNs and LSTMs. This parallelism leads to significant efficiency improvements, especially for long sequences, and makes training on modern hardware accelerators like GPUs and TPUs more effective.\\n\\nMoreover, self-attention models outperform RNNs/LSTMs in handling long-range dependencies because they do not rely on stepwise propagation of information across tokens. RNN-based architectures often struggle with vanishing gradients and fixed-length memory, which limit their ability to remember distant context. Self-attention bypasses these issues by directly modelling token-to-token interactions, enabling both better gradient flow and more flexible context utilization.\\n\\nIn summary, self-attention is the core mechanism enabling transformers to efficiently and effectively understand complex sequence data. It unifies context modeling across all positions, facilitates parallel execution, and overcomes legacy issues in sequential models, making it indispensable for modern deep learning sequence tasks.\\n\\n[[IMAGE_1]]\\n\\n## Mathematics Behind the Self-Attention Mechanism\\n\\nAt the core of the transformer architecture is the self-attention mechanism, which dynamically computes contextual relationships within an input sequence. Understanding its mathematics is crucial for implementing and debugging transformers effectively.\\n\\n### Input Representations: Query, Key, and Value Matrices\\n\\nThe self-attention mechanism operates on three distinct but related representations derived from the input embeddings (typically word or token embeddings):\\n\\n- **Query (Q):** Represents the set of vectors the model uses to query the input sequence.\\n- **Key (K):** Represents the sequence elements that each query will attend to.\\n- **Value (V):** Contains the actual data to be aggregated based on attention scores.\\n\\nUsually, these are obtained by multiplying the input embedding matrix \\\\( X \\\\in \\\\mathbb{R}^{n \\\\times d_{model}} \\\\), where \\\\( n \\\\) is the sequence length and \\\\( d_{model} \\\\) the embedding dimension, by learned projection matrices \\\\( W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d_{model} \\\\times d_k} \\\\):\\n\\n\\\\[\\nQ = X W_Q, \\\\quad K = X W_K, \\\\quad V = X W_V\\n\\\\]\\n\\nHere, \\\\( d_k \\\\) is the dimensionality of the queries and keys, often set such that \\\\( d_k = d_v = d_{model} / h \\\\) when using multiple attention heads (\\\\( h \\\\)).\\n\\n### Calculating Attention Scores\\n\\nAttention scores quantify the compatibility between each Query and Key pair via the dot product:\\n\\n\\\\[\\n\\\\text{scores} = Q K^\\\\top\\n\\\\]\\n\\nSince \\\\( Q \\\\in \\\\mathbb{R}^{n \\\\times d_k} \\\\) and \\\\( K^\\\\top \\\\in \\\\mathbb{R}^{d_k \\\\times n} \\\\), the resulting \\\\( \\\\text{scores} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) matrix expresses how much each token should attend to every other token.\\n\\nTo stabilize gradients and prevent excessively large dot products, scores are scaled by \\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_scores} = \\\\frac{Q K^\\\\top}{\\\\sqrt{d_k}}\\n\\\\]\\n\\nThis scaling is essential because the magnitude of dot products grows with \\\\( d_k \\\\), which can lead to softmax saturation and vanishing gradients.\\n\\n### Softmax Normalization and Attention Weights\\n\\nThe scaled scores are normalized using the softmax function along each Query\\'s dimension to produce attention weights:\\n\\n\\\\[\\n\\\\text{attention\\\\_weights}_{i,j} = \\\\frac{\\\\exp(\\\\text{scaled\\\\_scores}_{i,j})}{\\\\sum_{k=1}^{n} \\\\exp(\\\\text{scaled\\\\_scores}_{i,k})}\\n\\\\]\\n\\nThis ensures that for each query \\\\( i \\\\), the weights sum to 1, forming a valid probability distribution over keys \\\\( j \\\\). Normalization is critical to emphasize the most relevant keys while reducing noise from less relevant ones.\\n\\n### Weighted Sum Producing the Output Representation\\n\\nFinally, the output for each position is a weighted sum of the Values \\\\( V \\\\), using the computed attention weights:\\n\\n\\\\[\\n\\\\text{output} = \\\\text{attention\\\\_weights} \\\\times V\\n\\\\]\\n\\nGiven \\\\( \\\\text{attention\\\\_weights} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) and \\\\( V \\\\in \\\\mathbb{R}^{n \\\\times d_v} \\\\), the output has shape \\\\( n \\\\times d_v \\\\), providing contextually enriched representations for each token by aggregating information based on learned attention patterns.\\n\\n### Summary of Shapes and Flow\\n\\n| Variable           | Shape                      | Description                          |\\n|--------------------|----------------------------|------------------------------------|\\n| \\\\( X \\\\)            | \\\\( n \\\\times d_{model} \\\\)    | Input embeddings                   |\\n| \\\\( W_Q, W_K, W_V \\\\) | \\\\( d_{model} \\\\times d_k \\\\)  | Learned projection matrices        |\\n| \\\\( Q, K, V \\\\)      | \\\\( n \\\\times d_k \\\\)          | Projected Queries, Keys, Values    |\\n| Scores             | \\\\( n \\\\times n \\\\)            | Dot product of Q and \\\\( K^\\\\top \\\\)  |\\n| Attention Weights  | \\\\( n \\\\times n \\\\)            | Softmax-normalized scores          |\\n| Output             | \\\\( n \\\\times d_v \\\\)          | Weighted sum of values             |\\n\\n### Minimal Working Example in PyTorch\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef self_attention(X, W_Q, W_K, W_V):\\n    \"\"\"\\n    Computes self-attention output for input tensor X.\\n\\n    Args:\\n        X: Input embeddings, shape (n, d_model)\\n        W_Q, W_K, W_V: Projection matrices, shape (d_model, d_k)\\n\\n    Returns:\\n        output: Self-attention output, shape (n, d_k)\\n    \"\"\"\\n    Q = X @ W_Q            # (n, d_k)\\n    K = X @ W_K            # (n, d_k)\\n    V = X @ W_V            # (n, d_k)\\n\\n    d_k = Q.shape[-1]\\n    scores = Q @ K.T       # (n, n)\\n    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n\\n    attention_weights = F.softmax(scaled_scores, dim=1)  # Normalize rows\\n    output = attention_weights @ V                        # (n, d_k)\\n\\n    return output\\n\\n# Example usage:\\nn, d_model, d_k = 5, 512, 64\\nX = torch.rand(n, d_model)\\nW_Q = torch.rand(d_model, d_k)\\nW_K = torch.rand(d_model, d_k)\\nW_V = torch.rand(d_model, d_k)\\n\\noutput = self_attention(X, W_Q, W_K, W_V)\\nprint(output.shape)  # Expected: (5, 64)\\n```\\n\\nThis example highlights how matrix multiplications and softmax normalization are combined to implement self-attention. Understanding these steps allows you to debug transformers effectively, especially checking shapes at each step and ensuring proper scaling before softmax.\\n\\n## Minimal Code Example Demonstrating Self-Attention Computation\\n\\nTo concretize the concept of self-attention and facilitate implementation, here is a minimal runnable example of self-attention using PyTorch. This example explicitly maps inputs to queries (Q), keys (K), and values (V), computes scaled dot-product attention scores, applies the softmax normalization, and produces the final output.\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\n# Define input: batch_size=1, seq_len=3, embed_dim=4\\nx = torch.tensor([[[1., 0., 1., 0.],\\n                   [0., 2., 0., 2.],\\n                   [1., 1., 1., 1.]]])  # shape: (1, 3, 4)\\n\\n# Initialize simple linear layers to create Q, K, V from input\\n# Here weights are identity matrices for clarity\\nW_q = torch.eye(4)\\nW_k = torch.eye(4)\\nW_v = torch.eye(4)\\n\\n# Compute Q, K, V\\nQ = torch.matmul(x, W_q)  # shape: (1, 3, 4)\\nK = torch.matmul(x, W_k)  # shape: (1, 3, 4)\\nV = torch.matmul(x, W_v)  # shape: (1, 3, 4)\\n\\n# Step 1: Compute attention scores with scaled dot product\\nd_k = Q.size(-1)  # embedding dimension\\nscores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n# scores shape: (1, 3, 3)\\n\\n# Step 2: Apply softmax to obtain attention weights\\nattn_weights = F.softmax(scores, dim=-1)  # shape: (1, 3, 3)\\n\\n# Step 3: Compute output by weighted sum of values\\noutput = torch.matmul(attn_weights, V)  # shape: (1, 3, 4)\\n\\nprint(\"Attention scores:\\\\n\", scores)\\nprint(\"Attention weights (after softmax):\\\\n\", attn_weights)\\nprint(\"Self-attention output:\\\\n\", output)\\n```\\n\\n### Explanation and Verification\\n\\n- **Mapping Inputs to Q, K, V**: Here, for didactic purposes, we use identity matrices as weights, making Q, K, and V identical to the input. This simplifies understanding of the attention computation.\\n- **Scaled Dot-Product Calculation**: Scores are computed by multiplying Q with the transpose of K and scaling by the square root of the embedding dimension, which stabilizes gradients during training.\\n- **Softmax Application**: The softmax converts raw scores into probabilities that sum to 1 across the sequence length dimension, reflecting how much attention each token pays to others.\\n- **Output Computation**: The weighted sum of values V, weighted by attention probabilities, produces the self-attention output.\\n\\n### Testing and Debugging Tips\\n\\n- Start with small, easily interpretable inputs like above to manually verify intermediate values.\\n- Check shapes at every step to ensure matrix multiplications are consistent.\\n- Verify that each row of `attn_weights` sums to 1 (due to softmax).\\n- Test edge cases such as identical input vectors or zero vectors to observe expected outputs, e.g., uniform attention distributions or zeros.\\n- Compare outputs with manually computed expected values or use PyTorch’s built-in `nn.MultiheadAttention` as a reference to debug your implementation.\\n\\n[[IMAGE_2]]\\n\\n## Design Considerations and Performance Implications of Self-Attention\\n\\nSelf-attention lies at the core of transformer architectures, significantly shaping their design and performance characteristics. Understanding its computational behavior and resource implications is crucial for effective model development and deployment.\\n\\n### Computational Complexity\\n\\nThe self-attention mechanism computes attention scores between every pair of tokens in the input sequence. This results in a quadratic complexity with respect to the input length \\\\( N \\\\), specifically \\\\( O(N^2) \\\\). For each token, attention weights are calculated relative to all other tokens, creating an \\\\( N \\\\times N \\\\) attention matrix. While this enables rich contextual relations, it also leads to rapid growth in compute and memory requirements as input sequences lengthen.\\n\\n### Comparison with Recurrent Architectures\\n\\nUnlike recurrent neural networks (RNNs) that process tokens sequentially, self-attention processes the entire sequence simultaneously. This difference has two major impacts:\\n\\n- **Speed**: Self-attention enables parallel computation across all tokens at once, often resulting in faster training and inference times compared to RNNs that are inherently sequential.\\n- **Memory footprint**: The trade-off comes in memory use, where storing the attention matrix and intermediate activations can be substantially more demanding than the usually linear memory profile of RNNs.\\n\\n### Batching and Parallelization Advantages\\n\\nSelf-attention’s matrix operations lend themselves well to hardware acceleration. GPUs and TPUs can exploit massive parallelism when computing the dot products and softmax operations across the attention matrix. This capability allows batching of multiple sequences to maximize throughput, improving overall scalability. Parallelization both within and across sequences provides a strong practical advantage over recurrent alternatives, which often struggle to parallelize sequence steps.\\n\\n### Practical Constraints in Scaling\\n\\nDespite parallelizable computation, quadratic growth poses challenges for very long sequences. Common practical constraints include:\\n\\n- **Memory limits**: Large attention matrices become infeasible for inputs with thousands of tokens or more.\\n- **Latency considerations**: Slower performance due to larger matrix operations can affect real-time applications.\\n\\nTo cope, transformer implementations typically:\\n\\n- Limit maximum sequence lengths.\\n- Use sparse or approximate attention variants to reduce complexity.\\n- Apply techniques like windowing or chunking sequences.\\n\\n### Trade-offs Between Accuracy and Efficiency\\n\\nThe choice of attention mechanism involves balancing accuracy and computational cost. Full self-attention maximizes contextual relationships but at higher resource expense. Alternatives include:\\n\\n- **Sparse attention**: Restricting attention to local windows or fixed patterns reduces complexity to near linear but may miss global context.\\n- **Low-rank or kernel-based approximations**: These methods further reduce cost but can degrade model quality if not carefully tuned.\\n\\nModel designers must consider the application’s accuracy requirements alongside available hardware and latency constraints to select the appropriate attention variant, optimizing for both performance and resource efficiency.\\n\\n[[IMAGE_3]]\\n\\n## Common Edge Cases and Debugging Tips for Self-Attention Implementations\\n\\nWhen implementing self-attention, developers often encounter subtle bugs that can significantly degrade performance or cause runtime errors. Here are frequent errors and actionable debugging tips to help you build robust self-attention modules.\\n\\n### Frequent Errors to Watch For\\n- **Shape mismatches:** Queries, keys, and values must have compatible shapes. Remember the typical shape convention: `(batch_size, seq_length, embed_dim)`. Mixing up dimensions or batch vs. sequence axes is a common source of errors.\\n- **Incorrect scaling factor:** The attention logits should be scaled by `1 / sqrt(d_k)` where `d_k` is the dimensionality of the key vectors. Forgetting or miscalculating this results in poor gradient behavior.\\n- **Mishandling masked attention:** When applying masks for padding or causal attention, ensure the mask is broadcast correctly and applied before softmax to avoid leaking information or introducing NaNs.\\n\\n### Debugging with Intermediate Tensor Inspection\\nInsert debug statements to print or log intermediate tensors such as raw attention scores, post-mask logits, and attention weights. Visualizing these tensors can reveal unexpected values or distributions:\\n\\n```python\\nprint(\"Attention logits shape:\", logits.shape)\\nprint(\"Logits sample:\", logits[0, :5, :5])\\n```\\n\\nVisual tools like heatmaps can also help understand how attention weights are spread across tokens.\\n\\n### Verifying Numerical Stability\\nSoftmax can suffer overflow with large logits, leading to NaNs or Inf values:\\n\\n- Always subtract the max logit value from each logit vector before applying softmax:\\n  \\n  ```python\\n  logits = logits - logits.max(dim=-1, keepdim=True)[0]\\n  attention_weights = torch.softmax(logits, dim=-1)\\n  ```\\n\\n- Check for very large or very small logits during training to catch unstable inputs early.\\n\\n### Handling Variable-Length Sequences and Padding\\nTo correctly process batches with variable-length sequences:\\n\\n- Pad sequences to the same length.\\n- Create a mask that indicates valid tokens (`1`) vs. padding (`0`).\\n- Apply this mask by setting logits of padded tokens to a large negative number (e.g. `-1e9`) before softmax, ensuring they get negligible attention weights.\\n\\n### Unit Testing with Controlled Inputs\\nValidate your self-attention code using small, deterministic inputs where you can compute expected outputs by hand or from a reference implementation:\\n\\n- Test with identical queries and keys to verify attention peaks at correct positions.\\n- Use simple binary masks to check masking behavior.\\n- Compare shapes and verify softmax outputs sum to 1 along the correct axis.\\n\\n```python\\ndef test_attention_sum_to_one():\\n    queries = torch.ones(1, 3, 4)\\n    keys = torch.ones(1, 3, 4)\\n    values = torch.ones(1, 3, 4)\\n    mask = torch.tensor([[1,1,0]], dtype=torch.bool)\\n    attn_output, attn_weights = self_attention(queries, keys, values, mask)\\n    assert torch.allclose(attn_weights.sum(dim=-1), torch.tensor([1., 1., 0.]), atol=1e-5)\\n```\\n\\nThese targeted checks help catch common pitfalls early and ensure your self-attention layer behaves as expected in practice.',\n",
       " 'image_specs': [{'placeholder': '[[IMAGE_1]]',\n",
       "   'filename': 'self_attention_overview.png',\n",
       "   'alt': 'Self-attention mechanism overview diagram',\n",
       "   'caption': 'Diagram illustrating tokens attending to each other in a sequence via self-attention.',\n",
       "   'prompt': 'A technical diagram showing a sequence of tokens with arrows connecting each token to every other token, representing the self-attention mechanism in transformer models; tokens labeled as words or embeddings; style is clear, schematic and educational, with short labels',\n",
       "   'size': '1536x1024',\n",
       "   'quality': 'high'},\n",
       "  {'placeholder': '[[IMAGE_2]]',\n",
       "   'filename': 'self_attention_math_flow.png',\n",
       "   'alt': 'Mathematical flow of self-attention computation',\n",
       "   'caption': 'Flow diagram showing input embeddings transformed into Q, K, V, dot product calculation, scaling, softmax normalization, and weighted sum producing output.',\n",
       "   'prompt': 'A detailed flowchart diagram depicting the mathematical steps of self-attention: input embeddings transform into Query, Key, Value matrices, dot-product attention scores, scaling by root of d_k, softmax normalization, and weighted sum to output representations; with shapes annotated; schematic, clean, clear labeling',\n",
       "   'size': '1536x1024',\n",
       "   'quality': 'high'},\n",
       "  {'placeholder': '[[IMAGE_3]]',\n",
       "   'filename': 'self_attention_performance_tradeoffs.png',\n",
       "   'alt': 'Performance trade-offs in self-attention',\n",
       "   'caption': 'Diagram depicting quadratic complexity of self-attention vs. linear complexity of RNNs; parallel computation advantages and memory trade-offs; summary of scaling issues and mitigation techniques.',\n",
       "   'prompt': 'An infographic style diagram illustrating performance considerations of self-attention in transformers: quadratic complexity vs. RNN linear complexity, parallelization advantages, memory usage tradeoffs, and common mitigation techniques like sparse attention and chunking; clean, educational style with icons and annotations',\n",
       "   'size': '1536x1024',\n",
       "   'quality': 'high'}],\n",
       " 'final': '# Understanding Self-Attention in Transformer Architecture\\n\\n## Introduction to Self-Attention in Transformers\\n\\nSelf-attention is a specialized attention mechanism where a sequence\\'s elements attend to each other within the same input sequence, rather than relying on an external context as in traditional attention. Unlike classic attention, which typically relates one source sequence to a different target sequence (e.g., in encoder-decoder models), self-attention operates intra-sequence, allowing each token to dynamically weight and integrate information from all other tokens in that sequence.\\n\\nIn the transformer architecture, self-attention plays a crucial role in capturing dependencies between tokens regardless of their distance. This capability is essential for understanding context where relationships are non-local, such as in long sentences or documents. By calculating attention scores between every pair of tokens, the model learns to highlight relevant information across the entire input, enabling richer and more nuanced representations compared to simple fixed-window or sequential models.\\n\\nOne of the key practical benefits of self-attention is its suitability for parallel computation. Since dependencies among all tokens are processed simultaneously through matrix operations, transformers avoid the bottleneck of sequential token processing that characterizes RNNs and LSTMs. This parallelism leads to significant efficiency improvements, especially for long sequences, and makes training on modern hardware accelerators like GPUs and TPUs more effective.\\n\\nMoreover, self-attention models outperform RNNs/LSTMs in handling long-range dependencies because they do not rely on stepwise propagation of information across tokens. RNN-based architectures often struggle with vanishing gradients and fixed-length memory, which limit their ability to remember distant context. Self-attention bypasses these issues by directly modelling token-to-token interactions, enabling both better gradient flow and more flexible context utilization.\\n\\nIn summary, self-attention is the core mechanism enabling transformers to efficiently and effectively understand complex sequence data. It unifies context modeling across all positions, facilitates parallel execution, and overcomes legacy issues in sequential models, making it indispensable for modern deep learning sequence tasks.\\n\\n![Self-attention mechanism overview diagram](images/self_attention_overview.png)\\n*Diagram illustrating tokens attending to each other in a sequence via self-attention.*\\n\\n## Mathematics Behind the Self-Attention Mechanism\\n\\nAt the core of the transformer architecture is the self-attention mechanism, which dynamically computes contextual relationships within an input sequence. Understanding its mathematics is crucial for implementing and debugging transformers effectively.\\n\\n### Input Representations: Query, Key, and Value Matrices\\n\\nThe self-attention mechanism operates on three distinct but related representations derived from the input embeddings (typically word or token embeddings):\\n\\n- **Query (Q):** Represents the set of vectors the model uses to query the input sequence.\\n- **Key (K):** Represents the sequence elements that each query will attend to.\\n- **Value (V):** Contains the actual data to be aggregated based on attention scores.\\n\\nUsually, these are obtained by multiplying the input embedding matrix \\\\( X \\\\in \\\\mathbb{R}^{n \\\\times d_{model}} \\\\), where \\\\( n \\\\) is the sequence length and \\\\( d_{model} \\\\) the embedding dimension, by learned projection matrices \\\\( W_Q, W_K, W_V \\\\in \\\\mathbb{R}^{d_{model} \\\\times d_k} \\\\):\\n\\n\\\\[\\nQ = X W_Q, \\\\quad K = X W_K, \\\\quad V = X W_V\\n\\\\]\\n\\nHere, \\\\( d_k \\\\) is the dimensionality of the queries and keys, often set such that \\\\( d_k = d_v = d_{model} / h \\\\) when using multiple attention heads (\\\\( h \\\\)).\\n\\n### Calculating Attention Scores\\n\\nAttention scores quantify the compatibility between each Query and Key pair via the dot product:\\n\\n\\\\[\\n\\\\text{scores} = Q K^\\\\top\\n\\\\]\\n\\nSince \\\\( Q \\\\in \\\\mathbb{R}^{n \\\\times d_k} \\\\) and \\\\( K^\\\\top \\\\in \\\\mathbb{R}^{d_k \\\\times n} \\\\), the resulting \\\\( \\\\text{scores} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) matrix expresses how much each token should attend to every other token.\\n\\nTo stabilize gradients and prevent excessively large dot products, scores are scaled by \\\\( \\\\frac{1}{\\\\sqrt{d_k}} \\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_scores} = \\\\frac{Q K^\\\\top}{\\\\sqrt{d_k}}\\n\\\\]\\n\\nThis scaling is essential because the magnitude of dot products grows with \\\\( d_k \\\\), which can lead to softmax saturation and vanishing gradients.\\n\\n### Softmax Normalization and Attention Weights\\n\\nThe scaled scores are normalized using the softmax function along each Query\\'s dimension to produce attention weights:\\n\\n\\\\[\\n\\\\text{attention\\\\_weights}_{i,j} = \\\\frac{\\\\exp(\\\\text{scaled\\\\_scores}_{i,j})}{\\\\sum_{k=1}^{n} \\\\exp(\\\\text{scaled\\\\_scores}_{i,k})}\\n\\\\]\\n\\nThis ensures that for each query \\\\( i \\\\), the weights sum to 1, forming a valid probability distribution over keys \\\\( j \\\\). Normalization is critical to emphasize the most relevant keys while reducing noise from less relevant ones.\\n\\n### Weighted Sum Producing the Output Representation\\n\\nFinally, the output for each position is a weighted sum of the Values \\\\( V \\\\), using the computed attention weights:\\n\\n\\\\[\\n\\\\text{output} = \\\\text{attention\\\\_weights} \\\\times V\\n\\\\]\\n\\nGiven \\\\( \\\\text{attention\\\\_weights} \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) and \\\\( V \\\\in \\\\mathbb{R}^{n \\\\times d_v} \\\\), the output has shape \\\\( n \\\\times d_v \\\\), providing contextually enriched representations for each token by aggregating information based on learned attention patterns.\\n\\n### Summary of Shapes and Flow\\n\\n| Variable           | Shape                      | Description                          |\\n|--------------------|----------------------------|------------------------------------|\\n| \\\\( X \\\\)            | \\\\( n \\\\times d_{model} \\\\)    | Input embeddings                   |\\n| \\\\( W_Q, W_K, W_V \\\\) | \\\\( d_{model} \\\\times d_k \\\\)  | Learned projection matrices        |\\n| \\\\( Q, K, V \\\\)      | \\\\( n \\\\times d_k \\\\)          | Projected Queries, Keys, Values    |\\n| Scores             | \\\\( n \\\\times n \\\\)            | Dot product of Q and \\\\( K^\\\\top \\\\)  |\\n| Attention Weights  | \\\\( n \\\\times n \\\\)            | Softmax-normalized scores          |\\n| Output             | \\\\( n \\\\times d_v \\\\)          | Weighted sum of values             |\\n\\n### Minimal Working Example in PyTorch\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\ndef self_attention(X, W_Q, W_K, W_V):\\n    \"\"\"\\n    Computes self-attention output for input tensor X.\\n\\n    Args:\\n        X: Input embeddings, shape (n, d_model)\\n        W_Q, W_K, W_V: Projection matrices, shape (d_model, d_k)\\n\\n    Returns:\\n        output: Self-attention output, shape (n, d_k)\\n    \"\"\"\\n    Q = X @ W_Q            # (n, d_k)\\n    K = X @ W_K            # (n, d_k)\\n    V = X @ W_V            # (n, d_k)\\n\\n    d_k = Q.shape[-1]\\n    scores = Q @ K.T       # (n, n)\\n    scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n\\n    attention_weights = F.softmax(scaled_scores, dim=1)  # Normalize rows\\n    output = attention_weights @ V                        # (n, d_k)\\n\\n    return output\\n\\n# Example usage:\\nn, d_model, d_k = 5, 512, 64\\nX = torch.rand(n, d_model)\\nW_Q = torch.rand(d_model, d_k)\\nW_K = torch.rand(d_model, d_k)\\nW_V = torch.rand(d_model, d_k)\\n\\noutput = self_attention(X, W_Q, W_K, W_V)\\nprint(output.shape)  # Expected: (5, 64)\\n```\\n\\nThis example highlights how matrix multiplications and softmax normalization are combined to implement self-attention. Understanding these steps allows you to debug transformers effectively, especially checking shapes at each step and ensuring proper scaling before softmax.\\n\\n## Minimal Code Example Demonstrating Self-Attention Computation\\n\\nTo concretize the concept of self-attention and facilitate implementation, here is a minimal runnable example of self-attention using PyTorch. This example explicitly maps inputs to queries (Q), keys (K), and values (V), computes scaled dot-product attention scores, applies the softmax normalization, and produces the final output.\\n\\n```python\\nimport torch\\nimport torch.nn.functional as F\\n\\n# Define input: batch_size=1, seq_len=3, embed_dim=4\\nx = torch.tensor([[[1., 0., 1., 0.],\\n                   [0., 2., 0., 2.],\\n                   [1., 1., 1., 1.]]])  # shape: (1, 3, 4)\\n\\n# Initialize simple linear layers to create Q, K, V from input\\n# Here weights are identity matrices for clarity\\nW_q = torch.eye(4)\\nW_k = torch.eye(4)\\nW_v = torch.eye(4)\\n\\n# Compute Q, K, V\\nQ = torch.matmul(x, W_q)  # shape: (1, 3, 4)\\nK = torch.matmul(x, W_k)  # shape: (1, 3, 4)\\nV = torch.matmul(x, W_v)  # shape: (1, 3, 4)\\n\\n# Step 1: Compute attention scores with scaled dot product\\nd_k = Q.size(-1)  # embedding dimension\\nscores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\\n# scores shape: (1, 3, 3)\\n\\n# Step 2: Apply softmax to obtain attention weights\\nattn_weights = F.softmax(scores, dim=-1)  # shape: (1, 3, 3)\\n\\n# Step 3: Compute output by weighted sum of values\\noutput = torch.matmul(attn_weights, V)  # shape: (1, 3, 4)\\n\\nprint(\"Attention scores:\\\\n\", scores)\\nprint(\"Attention weights (after softmax):\\\\n\", attn_weights)\\nprint(\"Self-attention output:\\\\n\", output)\\n```\\n\\n### Explanation and Verification\\n\\n- **Mapping Inputs to Q, K, V**: Here, for didactic purposes, we use identity matrices as weights, making Q, K, and V identical to the input. This simplifies understanding of the attention computation.\\n- **Scaled Dot-Product Calculation**: Scores are computed by multiplying Q with the transpose of K and scaling by the square root of the embedding dimension, which stabilizes gradients during training.\\n- **Softmax Application**: The softmax converts raw scores into probabilities that sum to 1 across the sequence length dimension, reflecting how much attention each token pays to others.\\n- **Output Computation**: The weighted sum of values V, weighted by attention probabilities, produces the self-attention output.\\n\\n### Testing and Debugging Tips\\n\\n- Start with small, easily interpretable inputs like above to manually verify intermediate values.\\n- Check shapes at every step to ensure matrix multiplications are consistent.\\n- Verify that each row of `attn_weights` sums to 1 (due to softmax).\\n- Test edge cases such as identical input vectors or zero vectors to observe expected outputs, e.g., uniform attention distributions or zeros.\\n- Compare outputs with manually computed expected values or use PyTorch’s built-in `nn.MultiheadAttention` as a reference to debug your implementation.\\n\\n![Mathematical flow of self-attention computation](images/self_attention_math_flow.png)\\n*Flow diagram showing input embeddings transformed into Q, K, V, dot product calculation, scaling, softmax normalization, and weighted sum producing output.*\\n\\n## Design Considerations and Performance Implications of Self-Attention\\n\\nSelf-attention lies at the core of transformer architectures, significantly shaping their design and performance characteristics. Understanding its computational behavior and resource implications is crucial for effective model development and deployment.\\n\\n### Computational Complexity\\n\\nThe self-attention mechanism computes attention scores between every pair of tokens in the input sequence. This results in a quadratic complexity with respect to the input length \\\\( N \\\\), specifically \\\\( O(N^2) \\\\). For each token, attention weights are calculated relative to all other tokens, creating an \\\\( N \\\\times N \\\\) attention matrix. While this enables rich contextual relations, it also leads to rapid growth in compute and memory requirements as input sequences lengthen.\\n\\n### Comparison with Recurrent Architectures\\n\\nUnlike recurrent neural networks (RNNs) that process tokens sequentially, self-attention processes the entire sequence simultaneously. This difference has two major impacts:\\n\\n- **Speed**: Self-attention enables parallel computation across all tokens at once, often resulting in faster training and inference times compared to RNNs that are inherently sequential.\\n- **Memory footprint**: The trade-off comes in memory use, where storing the attention matrix and intermediate activations can be substantially more demanding than the usually linear memory profile of RNNs.\\n\\n### Batching and Parallelization Advantages\\n\\nSelf-attention’s matrix operations lend themselves well to hardware acceleration. GPUs and TPUs can exploit massive parallelism when computing the dot products and softmax operations across the attention matrix. This capability allows batching of multiple sequences to maximize throughput, improving overall scalability. Parallelization both within and across sequences provides a strong practical advantage over recurrent alternatives, which often struggle to parallelize sequence steps.\\n\\n### Practical Constraints in Scaling\\n\\nDespite parallelizable computation, quadratic growth poses challenges for very long sequences. Common practical constraints include:\\n\\n- **Memory limits**: Large attention matrices become infeasible for inputs with thousands of tokens or more.\\n- **Latency considerations**: Slower performance due to larger matrix operations can affect real-time applications.\\n\\nTo cope, transformer implementations typically:\\n\\n- Limit maximum sequence lengths.\\n- Use sparse or approximate attention variants to reduce complexity.\\n- Apply techniques like windowing or chunking sequences.\\n\\n### Trade-offs Between Accuracy and Efficiency\\n\\nThe choice of attention mechanism involves balancing accuracy and computational cost. Full self-attention maximizes contextual relationships but at higher resource expense. Alternatives include:\\n\\n- **Sparse attention**: Restricting attention to local windows or fixed patterns reduces complexity to near linear but may miss global context.\\n- **Low-rank or kernel-based approximations**: These methods further reduce cost but can degrade model quality if not carefully tuned.\\n\\nModel designers must consider the application’s accuracy requirements alongside available hardware and latency constraints to select the appropriate attention variant, optimizing for both performance and resource efficiency.\\n\\n![Performance trade-offs in self-attention](images/self_attention_performance_tradeoffs.png)\\n*Diagram depicting quadratic complexity of self-attention vs. linear complexity of RNNs; parallel computation advantages and memory trade-offs; summary of scaling issues and mitigation techniques.*\\n\\n## Common Edge Cases and Debugging Tips for Self-Attention Implementations\\n\\nWhen implementing self-attention, developers often encounter subtle bugs that can significantly degrade performance or cause runtime errors. Here are frequent errors and actionable debugging tips to help you build robust self-attention modules.\\n\\n### Frequent Errors to Watch For\\n- **Shape mismatches:** Queries, keys, and values must have compatible shapes. Remember the typical shape convention: `(batch_size, seq_length, embed_dim)`. Mixing up dimensions or batch vs. sequence axes is a common source of errors.\\n- **Incorrect scaling factor:** The attention logits should be scaled by `1 / sqrt(d_k)` where `d_k` is the dimensionality of the key vectors. Forgetting or miscalculating this results in poor gradient behavior.\\n- **Mishandling masked attention:** When applying masks for padding or causal attention, ensure the mask is broadcast correctly and applied before softmax to avoid leaking information or introducing NaNs.\\n\\n### Debugging with Intermediate Tensor Inspection\\nInsert debug statements to print or log intermediate tensors such as raw attention scores, post-mask logits, and attention weights. Visualizing these tensors can reveal unexpected values or distributions:\\n\\n```python\\nprint(\"Attention logits shape:\", logits.shape)\\nprint(\"Logits sample:\", logits[0, :5, :5])\\n```\\n\\nVisual tools like heatmaps can also help understand how attention weights are spread across tokens.\\n\\n### Verifying Numerical Stability\\nSoftmax can suffer overflow with large logits, leading to NaNs or Inf values:\\n\\n- Always subtract the max logit value from each logit vector before applying softmax:\\n  \\n  ```python\\n  logits = logits - logits.max(dim=-1, keepdim=True)[0]\\n  attention_weights = torch.softmax(logits, dim=-1)\\n  ```\\n\\n- Check for very large or very small logits during training to catch unstable inputs early.\\n\\n### Handling Variable-Length Sequences and Padding\\nTo correctly process batches with variable-length sequences:\\n\\n- Pad sequences to the same length.\\n- Create a mask that indicates valid tokens (`1`) vs. padding (`0`).\\n- Apply this mask by setting logits of padded tokens to a large negative number (e.g. `-1e9`) before softmax, ensuring they get negligible attention weights.\\n\\n### Unit Testing with Controlled Inputs\\nValidate your self-attention code using small, deterministic inputs where you can compute expected outputs by hand or from a reference implementation:\\n\\n- Test with identical queries and keys to verify attention peaks at correct positions.\\n- Use simple binary masks to check masking behavior.\\n- Compare shapes and verify softmax outputs sum to 1 along the correct axis.\\n\\n```python\\ndef test_attention_sum_to_one():\\n    queries = torch.ones(1, 3, 4)\\n    keys = torch.ones(1, 3, 4)\\n    values = torch.ones(1, 3, 4)\\n    mask = torch.tensor([[1,1,0]], dtype=torch.bool)\\n    attn_output, attn_weights = self_attention(queries, keys, values, mask)\\n    assert torch.allclose(attn_weights.sum(dim=-1), torch.tensor([1., 1., 0.]), atol=1e-5)\\n```\\n\\nThese targeted checks help catch common pitfalls early and ensure your self-attention layer behaves as expected in practice.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"Self Attention in Transformer Architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9022798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
